{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Selamat datang dihalaman tugas Penambangan Data \u00b6 Nama : Lailatul Badria D.A.A Nim : 180411100149 Kelas : Penambangan Data-5B Jurusan : Teknik Informatika Angkatan : 2018 Perguruan Tinggi : Universitas Trunojoyo Madura","title":"Index"},{"location":"#selamat-datang-dihalaman-tugas-penambangan-data","text":"Nama : Lailatul Badria D.A.A Nim : 180411100149 Kelas : Penambangan Data-5B Jurusan : Teknik Informatika Angkatan : 2018 Perguruan Tinggi : Universitas Trunojoyo Madura","title":"Selamat datang dihalaman tugas Penambangan Data"},{"location":"Home/","text":"Komputasi Numerik \u00b6 Nama : Lailatul Badria D.A.A Nim : 180411100149 Kelas : Komputasi Numerik-4B Jurusan : Teknik Informatika Angkatan : 2018 Perguruan Tinggi : Universitas Trunojoyo Madura","title":"Home"},{"location":"Home/#komputasi-numerik","text":"Nama : Lailatul Badria D.A.A Nim : 180411100149 Kelas : Komputasi Numerik-4B Jurusan : Teknik Informatika Angkatan : 2018 Perguruan Tinggi : Universitas Trunojoyo Madura","title":"Komputasi Numerik"},{"location":"error/","text":"Error in numerical computation \u00b6 ERROR \u00b6 \u200b Error merupakan perbedaan antara hasil penyelesaian suatu model matematik secara numeric dengan penyelesaian secara analitis. Kesalahan yang terjadi sangatlah penting, karena kesalahan dalam pemakaian algoritma pendekatan akan menyebabkan nilai kesalahan yang besar. Sehingga pendekatan metode numerik selalu membahas tingkat kesalahan dan tingkat kecepatan proses yang akan terjadi. NILAI ERROR \u00b6 \u200b Besarnya kesalahan atas suatu nilai taksiran dapat dinyatakan secara kuantitatif dan kualitatif. Besarnya kesalahan yang dinyatakan secara kuantitatif disebut Kesalahan Absolut. Besarnya kesalahan yang dinyatakan secara kualitatif disebut dengan kesalahan Relatif . Absolut error \u00b6 \u200b Kesalahan absolut suatu kuantitas adalah nilai absolut dari selisih antara nilai sebenarnya X dan nilai perkiraan x. Ini dilambangkan dengan $$ Ea = |X - x| $$ kesalahan Relatif \u00b6 \u200b Relative error biasa disebut sebagai kesalahan relatif dari suatu kuantitas adalah rasio kesalahan absolutnya terhadap nilai sebenarnya. Ini dilambangkan dengan Er. $$ Er = |Xt - Xa / Xt| $$ PENYEBAB TERJADINYA ERROR \u00b6 Dibedakan dalam beberapa macam : 1.Round-off-errors \u00b6 \u200b Perhitungan dengan metode numerik hampir selalu menggunakan bilanganriil.Masalah timbul bila komputasi numerik dikerjakan oleh mesin (dalam hal ini komputer) karena semua bilangan riil tidak dapat disajikan secara tepat di dalamkomputer. Keterbatasan komputer dalam menyajikan bilangan riil menghasilkangalat yang disebut galat pembulatan . Sebagai contoh 1/6 = 0.166666666\u2026 tidak dapat dinyatakan secara tepat oleh komputer karena digit 6 panjangnya tidak terbatas. Komputer hanya mampu merepresentasikan sejumlah digit (atau bit dalam sistem biner) saja. 2.Truncation errors \u00b6 Kesalahan pemotongan terjadi ketika suatu rumus komputasi disederhanakan dengan cara membuang suku yang berderajat tinggi. True Error Didefinisikan sebagai beda antara nilai eksak dalam penghitungan dan pendekatan menggunakan metode numerik. 3.Inherent error \u00b6 DEFINISI MACLAURIN \u00b6 \u200b Suatu fungsi f(x) yang memiliki turunan , , , dan seterusnya yang kontinyu dalam interval dengan maka untuk disekitar yaitu , dapat diekspansi kedalam Deret TaylorDefinisi. Berikut algoritma dari maclaurin \u00b6 Dengan algoritma diatas kita dapat menyerderhanakannya sebagai berikut: berikut contoh implementai dari maclaurin f(x)= e 2x $$ f(x)\u22481+2x \\displaystyle+\\frac{{{{f}^{{\\text{}}}{\\left({2x^2}\\right)}}}}{{{3}!}} \\displaystyle+\\frac{{{{f}^{{\\text{}}}{\\left({2x^3}\\right)}}}}{{{3}!}} \\displaystyle+\\ldots+\u2026 $$ sekarang kita masukan misal x=0 $$ f(0)\u22481+2(0) \\displaystyle+\\frac{{{{}^{{\\text{}}}{\\left({2(0)^2}\\right)}}}}{{{3}!}} \\displaystyle+\\frac{{{{}^{{\\text{}}}{\\left({2(0)^3}\\right)}}}}{{{3}!}} \\displaystyle+\\ldots+\u2026 $$ jadi ketika x =0 maka hasil akan tetap 1 mekipun banyak suku dan literasi Listing Program \u00b6 membuat program supaya dapaat mengekspansi bilangan e^3x dengan nilai x=4 hingga nilai menjadi kurang dari 0,001 bisa dengan listing program sebagai berikut. import math coba = 1 a = 0 b = 1 x = int ( input ( \"masukkan x = \" )) while coba > 0.001 : f_x = 0 f_y = 0 for i in range ( a ): f_x += ( 2 ** i ) * x ** i / math . factorial ( i ) for j in range ( b ): f_y += ( 2 ** j ) * x ** j / math . factorial ( j ) print ( \"suku ke \" , a , \"=\" , f_x ) print ( \"suku ke \" , b , \"=\" , f_y ) coba = f_y - f_x a += 1 b += 1 print ( \"selisih sukunya = \" , coba ) output: masukkan x = 1 suku ke 0 = 0 suku ke 1 = 1.0 selisih sukunya = 1.0 suku ke 1 = 1.0 suku ke 2 = 3.0 selisih sukunya = 2.0 suku ke 2 = 3.0 suku ke 3 = 5.0 selisih sukunya = 2.0 suku ke 3 = 5.0 suku ke 4 = 6.333333333333333 selisih sukunya = 1.333333333333333 suku ke 4 = 6.333333333333333 suku ke 5 = 7.0 selisih sukunya = 0.666666666666667 suku ke 5 = 7.0 suku ke 6 = 7.266666666666667 selisih sukunya = 0.2666666666666666 suku ke 6 = 7.266666666666667 suku ke 7 = 7.355555555555555 selisih sukunya = 0.08888888888888857 suku ke 7 = 7.355555555555555 suku ke 8 = 7.3809523809523805 selisih sukunya = 0.025396825396825307 suku ke 8 = 7.3809523809523805 suku ke 9 = 7.387301587301587 selisih sukunya = 0.006349206349206327 suku ke 9 = 7.387301587301587 suku ke 10 = 7.3887125220458545 selisih sukunya = 0.0014109347442676778 suku ke 10 = 7.3887125220458545 suku ke 11 = 7.388994708994708 selisih sukunya = 0.0002821869488531803 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$'],['$','$']]} });","title":"Error"},{"location":"error/#error-in-numerical-computation","text":"","title":"Error in numerical computation"},{"location":"error/#error","text":"\u200b Error merupakan perbedaan antara hasil penyelesaian suatu model matematik secara numeric dengan penyelesaian secara analitis. Kesalahan yang terjadi sangatlah penting, karena kesalahan dalam pemakaian algoritma pendekatan akan menyebabkan nilai kesalahan yang besar. Sehingga pendekatan metode numerik selalu membahas tingkat kesalahan dan tingkat kecepatan proses yang akan terjadi.","title":"ERROR"},{"location":"error/#nilai-error","text":"\u200b Besarnya kesalahan atas suatu nilai taksiran dapat dinyatakan secara kuantitatif dan kualitatif. Besarnya kesalahan yang dinyatakan secara kuantitatif disebut Kesalahan Absolut. Besarnya kesalahan yang dinyatakan secara kualitatif disebut dengan kesalahan Relatif .","title":"NILAI ERROR"},{"location":"error/#absolut-error","text":"\u200b Kesalahan absolut suatu kuantitas adalah nilai absolut dari selisih antara nilai sebenarnya X dan nilai perkiraan x. Ini dilambangkan dengan $$ Ea = |X - x| $$","title":"Absolut error"},{"location":"error/#kesalahan-relatif","text":"\u200b Relative error biasa disebut sebagai kesalahan relatif dari suatu kuantitas adalah rasio kesalahan absolutnya terhadap nilai sebenarnya. Ini dilambangkan dengan Er. $$ Er = |Xt - Xa / Xt| $$","title":"kesalahan Relatif"},{"location":"error/#penyebab-terjadinya-error","text":"Dibedakan dalam beberapa macam :","title":"PENYEBAB TERJADINYA ERROR"},{"location":"error/#1round-off-errors","text":"\u200b Perhitungan dengan metode numerik hampir selalu menggunakan bilanganriil.Masalah timbul bila komputasi numerik dikerjakan oleh mesin (dalam hal ini komputer) karena semua bilangan riil tidak dapat disajikan secara tepat di dalamkomputer. Keterbatasan komputer dalam menyajikan bilangan riil menghasilkangalat yang disebut galat pembulatan . Sebagai contoh 1/6 = 0.166666666\u2026 tidak dapat dinyatakan secara tepat oleh komputer karena digit 6 panjangnya tidak terbatas. Komputer hanya mampu merepresentasikan sejumlah digit (atau bit dalam sistem biner) saja.","title":"1.Round-off-errors"},{"location":"error/#2truncation-errors","text":"Kesalahan pemotongan terjadi ketika suatu rumus komputasi disederhanakan dengan cara membuang suku yang berderajat tinggi. True Error Didefinisikan sebagai beda antara nilai eksak dalam penghitungan dan pendekatan menggunakan metode numerik.","title":"2.Truncation errors"},{"location":"error/#3inherent-error","text":"","title":"3.Inherent error"},{"location":"error/#definisi-maclaurin","text":"\u200b Suatu fungsi f(x) yang memiliki turunan , , , dan seterusnya yang kontinyu dalam interval dengan maka untuk disekitar yaitu , dapat diekspansi kedalam Deret TaylorDefinisi.","title":"DEFINISI MACLAURIN"},{"location":"error/#berikut-algoritma-dari-maclaurin","text":"Dengan algoritma diatas kita dapat menyerderhanakannya sebagai berikut: berikut contoh implementai dari maclaurin f(x)= e 2x $$ f(x)\u22481+2x \\displaystyle+\\frac{{{{f}^{{\\text{}}}{\\left({2x^2}\\right)}}}}{{{3}!}} \\displaystyle+\\frac{{{{f}^{{\\text{}}}{\\left({2x^3}\\right)}}}}{{{3}!}} \\displaystyle+\\ldots+\u2026 $$ sekarang kita masukan misal x=0 $$ f(0)\u22481+2(0) \\displaystyle+\\frac{{{{}^{{\\text{}}}{\\left({2(0)^2}\\right)}}}}{{{3}!}} \\displaystyle+\\frac{{{{}^{{\\text{}}}{\\left({2(0)^3}\\right)}}}}{{{3}!}} \\displaystyle+\\ldots+\u2026 $$ jadi ketika x =0 maka hasil akan tetap 1 mekipun banyak suku dan literasi","title":"Berikut algoritma dari maclaurin"},{"location":"error/#listing-program","text":"membuat program supaya dapaat mengekspansi bilangan e^3x dengan nilai x=4 hingga nilai menjadi kurang dari 0,001 bisa dengan listing program sebagai berikut. import math coba = 1 a = 0 b = 1 x = int ( input ( \"masukkan x = \" )) while coba > 0.001 : f_x = 0 f_y = 0 for i in range ( a ): f_x += ( 2 ** i ) * x ** i / math . factorial ( i ) for j in range ( b ): f_y += ( 2 ** j ) * x ** j / math . factorial ( j ) print ( \"suku ke \" , a , \"=\" , f_x ) print ( \"suku ke \" , b , \"=\" , f_y ) coba = f_y - f_x a += 1 b += 1 print ( \"selisih sukunya = \" , coba ) output: masukkan x = 1 suku ke 0 = 0 suku ke 1 = 1.0 selisih sukunya = 1.0 suku ke 1 = 1.0 suku ke 2 = 3.0 selisih sukunya = 2.0 suku ke 2 = 3.0 suku ke 3 = 5.0 selisih sukunya = 2.0 suku ke 3 = 5.0 suku ke 4 = 6.333333333333333 selisih sukunya = 1.333333333333333 suku ke 4 = 6.333333333333333 suku ke 5 = 7.0 selisih sukunya = 0.666666666666667 suku ke 5 = 7.0 suku ke 6 = 7.266666666666667 selisih sukunya = 0.2666666666666666 suku ke 6 = 7.266666666666667 suku ke 7 = 7.355555555555555 selisih sukunya = 0.08888888888888857 suku ke 7 = 7.355555555555555 suku ke 8 = 7.3809523809523805 selisih sukunya = 0.025396825396825307 suku ke 8 = 7.3809523809523805 suku ke 9 = 7.387301587301587 selisih sukunya = 0.006349206349206327 suku ke 9 = 7.387301587301587 suku ke 10 = 7.3887125220458545 selisih sukunya = 0.0014109347442676778 suku ke 10 = 7.3887125220458545 suku ke 11 = 7.388994708994708 selisih sukunya = 0.0002821869488531803 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$'],['$','$']]} });","title":"Listing Program"},{"location":"jrak/","text":"Mengukur Jarak Data \u00b6 Mengukur Jarak Tipe Numerik \u00b6 Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkanSebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaitu v1,v2v1,v2 menyatakandua vektor yang menyatakan v1=x1,x2,...,xn,v2=y1,y2,...,yn,v1=x1,x2,...,xn,v2=y1,y2,...,yn, dimana xi,yixi,yi disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya Minkowski Distance \u00b6 Kelompk Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. $$ d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$ Manhattan distance \u00b6 Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan $$ d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| $$ Euclidean distance \u00b6 Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini. Average Distance \u00b6 Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,y dalam ruang dimensi n, rata-rata jarak didefinisikan dengan $$ d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } $$ Weighted euclidean distance \u00b6 Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$ Chord distance \u00b6 Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan $$ d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } $$ $$ dimana | x | {2} adalah L^{2} \\text {-norm} | x | {2} = \\sqrt { \\sum_{ i = 1 }^{ n }x_{i}^{2}} $$ Mahalanobis distance \u00b6 Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan $$ d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } $$ Tugas II Mengukur Jarak Data \u00b6 from scipy import stats import numpy as np import seaborn as sns import matplotlib.pyplot as plt import pandas as pd df = pd . read_csv ( 'data2.csv' , sep = \";\" ) k = df . iloc [ 10 : 17 ] k .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Age of patient at time of operation Patient's year of operation Number of positive axillary nodes detected Unnamed: 3 10 34 60 1 1 11 34 61 10 1 12 34 67 7 1 13 34 60 0 1 14 35 64 13 1 15 35 63 0 1 16 36 60 1 1 numerical = [ 0 , 3 ] categorical = [ 1 , 2 , 6 , 7 ] binary = [ 4 , 5 , 8 ] ordinal = [ 1 , 2 ] from IPython.display import HTML , display import tabulate table = [ [ \"Data\" ] + [ \"Jarak\" ] + [ \"Numeric\" ] + [ \"Ordinal\" ] + [ \"Categorical\" ] + [ \"Binary\" ], [ \"v1-v2\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v1-v3\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v2-v3\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v3-v4\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v4-v5\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v5-v6\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] ] display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' ))) Data Jarak Numeric Ordinal Categorical Binary v1-v2 0 0 0 0 0 v1-v3 0 0 0 0 0 v2-v3 0 0 0 0 0 v3-v4 0 0 0 0 0 v4-v5 0 0 0 0 0 v5-v6 0 0 0 0 0 Jarak numeric \u00b6 def chordDist ( v1 , v2 , jnis ): jmlh = 0 normv1 = 0 normv2 = 0 for x in range ( len ( jnis )): normv1 = normv1 + ( int ( k . values . tolist ()[ v1 ][ jnis [ x ]]) ** 2 ) normv2 = normv2 + ( int ( k . values . tolist ()[ v1 ][ jnis [ x ]]) ** 2 ) jmlh = jmlh + ( int ( k . values . tolist ()[ v1 ][ jnis [ x ]]) * int ( k . values . tolist ()[ v2 ][ jnis [ x ]])) return (( 2 - ( 2 * jmlh / ( normv1 * normv2 ))) ** 0.5 ) from IPython.display import HTML , display import tabulate table = [ [ \"Data\" ] + [ \"Jarak\" ] + [ \"Numeric\" ] + [ \"Ordinal\" ] + [ \"Categorical\" ] + [ \"Binary\" ], [ \"v1-v2\" ] + [ 0 ] + [ \" {:.2f} \" . format ( chordDist ( 0 , 1 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v1-v3\" ] + [ 0 ] + [ \" {:.2f} \" . format ( chordDist ( 0 , 2 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v2-v3\" ] + [ 0 ] + [ \" {:.2f} \" . format ( chordDist ( 1 , 2 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v3-v4\" ] + [ 0 ] + [ \" {:.2f} \" . format ( chordDist ( 2 , 3 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v4-v5\" ] + [ 0 ] + [ \" {:.2f} \" . format ( chordDist ( 3 , 4 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v5-v6\" ] + [ 0 ] + [ \" {:.2f} \" . format ( chordDist ( 2 , 3 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], ] display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' ))) Data Jarak Numeric Ordinal Categorical Binary v1-v2 0 1.41 0 0 0 v1-v3 0 1.41 0 0 0 v2-v3 0 1.41 0 0 0 v3-v4 0 1.41 0 0 0 v4-v5 0 1.41 0 0 0 v5-v6 0 1.41 0 0 0 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Mengukur jarak data"},{"location":"jrak/#mengukur-jarak-data","text":"","title":"Mengukur Jarak Data"},{"location":"jrak/#mengukur-jarak-tipe-numerik","text":"Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkanSebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaitu v1,v2v1,v2 menyatakandua vektor yang menyatakan v1=x1,x2,...,xn,v2=y1,y2,...,yn,v1=x1,x2,...,xn,v2=y1,y2,...,yn, dimana xi,yixi,yi disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya","title":"Mengukur Jarak Tipe Numerik"},{"location":"jrak/#minkowski-distance","text":"Kelompk Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. $$ d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$","title":"Minkowski Distance"},{"location":"jrak/#manhattan-distance","text":"Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan $$ d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| $$","title":"Manhattan distance"},{"location":"jrak/#euclidean-distance","text":"Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini.","title":"Euclidean distance"},{"location":"jrak/#average-distance","text":"Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,y dalam ruang dimensi n, rata-rata jarak didefinisikan dengan $$ d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } $$","title":"Average Distance"},{"location":"jrak/#weighted-euclidean-distance","text":"Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$","title":"Weighted euclidean distance"},{"location":"jrak/#chord-distance","text":"Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan $$ d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } $$ $$ dimana | x | {2} adalah L^{2} \\text {-norm} | x | {2} = \\sqrt { \\sum_{ i = 1 }^{ n }x_{i}^{2}} $$","title":"Chord distance"},{"location":"jrak/#mahalanobis-distance","text":"Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan $$ d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } $$","title":"Mahalanobis distance"},{"location":"jrak/#tugas-ii-mengukur-jarak-data","text":"from scipy import stats import numpy as np import seaborn as sns import matplotlib.pyplot as plt import pandas as pd df = pd . read_csv ( 'data2.csv' , sep = \";\" ) k = df . iloc [ 10 : 17 ] k .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Age of patient at time of operation Patient's year of operation Number of positive axillary nodes detected Unnamed: 3 10 34 60 1 1 11 34 61 10 1 12 34 67 7 1 13 34 60 0 1 14 35 64 13 1 15 35 63 0 1 16 36 60 1 1 numerical = [ 0 , 3 ] categorical = [ 1 , 2 , 6 , 7 ] binary = [ 4 , 5 , 8 ] ordinal = [ 1 , 2 ] from IPython.display import HTML , display import tabulate table = [ [ \"Data\" ] + [ \"Jarak\" ] + [ \"Numeric\" ] + [ \"Ordinal\" ] + [ \"Categorical\" ] + [ \"Binary\" ], [ \"v1-v2\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v1-v3\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v2-v3\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v3-v4\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v4-v5\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v5-v6\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] ] display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' ))) Data Jarak Numeric Ordinal Categorical Binary v1-v2 0 0 0 0 0 v1-v3 0 0 0 0 0 v2-v3 0 0 0 0 0 v3-v4 0 0 0 0 0 v4-v5 0 0 0 0 0 v5-v6 0 0 0 0 0","title":"Tugas II Mengukur Jarak Data"},{"location":"jrak/#jarak-numeric","text":"def chordDist ( v1 , v2 , jnis ): jmlh = 0 normv1 = 0 normv2 = 0 for x in range ( len ( jnis )): normv1 = normv1 + ( int ( k . values . tolist ()[ v1 ][ jnis [ x ]]) ** 2 ) normv2 = normv2 + ( int ( k . values . tolist ()[ v1 ][ jnis [ x ]]) ** 2 ) jmlh = jmlh + ( int ( k . values . tolist ()[ v1 ][ jnis [ x ]]) * int ( k . values . tolist ()[ v2 ][ jnis [ x ]])) return (( 2 - ( 2 * jmlh / ( normv1 * normv2 ))) ** 0.5 ) from IPython.display import HTML , display import tabulate table = [ [ \"Data\" ] + [ \"Jarak\" ] + [ \"Numeric\" ] + [ \"Ordinal\" ] + [ \"Categorical\" ] + [ \"Binary\" ], [ \"v1-v2\" ] + [ 0 ] + [ \" {:.2f} \" . format ( chordDist ( 0 , 1 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v1-v3\" ] + [ 0 ] + [ \" {:.2f} \" . format ( chordDist ( 0 , 2 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v2-v3\" ] + [ 0 ] + [ \" {:.2f} \" . format ( chordDist ( 1 , 2 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v3-v4\" ] + [ 0 ] + [ \" {:.2f} \" . format ( chordDist ( 2 , 3 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v4-v5\" ] + [ 0 ] + [ \" {:.2f} \" . format ( chordDist ( 3 , 4 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v5-v6\" ] + [ 0 ] + [ \" {:.2f} \" . format ( chordDist ( 2 , 3 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], ] display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' ))) Data Jarak Numeric Ordinal Categorical Binary v1-v2 0 1.41 0 0 0 v1-v3 0 1.41 0 0 0 v2-v3 0 1.41 0 0 0 v3-v4 0 1.41 0 0 0 v4-v5 0 1.41 0 0 0 v5-v6 0 1.41 0 0 0 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Jarak numeric"},{"location":"knn/","text":"Weight-KNN \u00b6 W-KNN (Weighted K-Nearest Neighbour) \u00b6 adalah versi modifikasi dari K-NN . Salah satu dari banyak masalah yang mempengaruhi kinerja algoritma kNN adalah pilihan hyperparameter k. Jika k terlalu kecil, algoritme akan lebih sensitif terhadap pencilan. Jika k terlalu besar, maka lingkungan tersebut mungkin memasukkan terlalu banyak poin dari kelas lain. Masalah lainnya adalah pendekatan untuk menggabungkan label kelas. Metode paling sederhana adalah dengan mengambil suara terbanyak, tetapi ini bisa menjadi masalah jika tetangga terdekat sangat bervariasi dalam jarak mereka dan tetangga terdekat lebih andal menunjukkan kelas objek. Algoritma \u00b6 Misalkan L = {(x i , y i ), i = 1,. . . , n} menjadi seperangkat pelatihan pengamatan x i dengan kelas yang diberikan y i dan membiarkan x menjadi pengamatan baru (titik kueri), yang label kelasnya y harus diprediksi. Hitung d (x i , x) untuk i = 1,. . . , n, jarak antara titik kueri dan setiap titik lainnya dalam set pelatihan. Pilih D '\u2286 D, set k poin data pelatihan terdekat ke poin kueri Memprediksi kelas titik kueri, menggunakan pemungutan suara berbobot jarak. V mewakili label kelas. Contoh : \u00b6 Data iris \u00b6 sepal length sepal width petal length petal width class 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa 5 5.4 3.9 1.7 0.4 Iris-setosa 6 4.6 3.4 1.4 0.3 Iris-setosa 7 5.0 3.4 1.5 0.2 Iris-setosa 8 4.4 2.9 1.4 0.2 Iris-setosa 9 4.9 3.1 1.5 0.1 Iris-setosa 10 5.4 3.7 1.5 0.2 Iris-setosa 11 4.8 3.4 1.6 0.2 Iris-setosa 12 4.8 3.0 1.4 0.1 Iris-setosa 13 4.3 3.0 1.1 0.1 Iris-setosa 14 5.8 4.0 1.2 0.2 Iris-setosa 15 5.7 4.4 1.5 0.4 Iris-setosa 16 5.4 3.9 1.3 0.4 Iris-setosa 17 5.1 3.5 1.4 0.3 Iris-setosa 18 5.7 3.8 1.7 0.3 Iris-setosa 19 5.1 3.8 1.5 0.3 Iris-setosa 20 5.4 3.4 1.7 0.2 Iris-setosa 21 5.1 3.7 1.5 0.4 Iris-setosa 22 4.6 3.6 1.0 0.2 Iris-setosa 23 5.1 3.3 1.7 0.5 Iris-setosa 24 4.8 3.4 1.9 0.2 Iris-setosa 25 5.0 3.0 1.6 0.2 Iris-setosa 26 5.0 3.4 1.6 0.4 Iris-setosa 27 5.2 3.5 1.5 0.2 Iris-setosa 28 5.2 3.4 1.4 0.2 Iris-setosa 29 4.7 3.2 1.6 0.2 Iris-setosa ... ... ... ... ... ... 120 6.9 3.2 5.7 2.3 Iris-virginica 121 5.6 2.8 4.9 2.0 Iris-virginica 122 7.7 2.8 6.7 2.0 Iris-virginica 123 6.3 2.7 4.9 1.8 Iris-virginica 124 6.7 3.3 5.7 2.1 Iris-virginica 125 7.2 3.2 6.0 1.8 Iris-virginica 126 6.2 2.8 4.8 1.8 Iris-virginica 127 6.1 3.0 4.9 1.8 Iris-virginica 128 6.4 2.8 5.6 2.1 Iris-virginica 129 7.2 3.0 5.8 1.6 Iris-virginica 130 7.4 2.8 6.1 1.9 Iris-virginica 131 7.9 3.8 6.4 2.0 Iris-virginica 132 6.4 2.8 5.6 2.2 Iris-virginica 133 6.3 2.8 5.1 1.5 Iris-virginica 134 6.1 2.6 5.6 1.4 Iris-virginica 135 7.7 3.0 6.1 2.3 Iris-virginica 136 6.3 3.4 5.6 2.4 Iris-virginica 137 6.4 3.1 5.5 1.8 Iris-virginica 138 6.0 3.0 4.8 1.8 Iris-virginica 139 6.9 3.1 5.4 2.1 Iris-virginica 140 6.7 3.1 5.6 2.4 Iris-virginica 141 6.9 3.1 5.1 2.3 Iris-virginica 142 5.8 2.7 5.1 1.9 Iris-virginica 143 6.8 3.2 5.9 2.3 Iris-virginica 144 6.7 3.3 5.7 2.5 Iris-virginica 145 6.7 3.0 5.2 2.3 Iris-virginica 146 6.3 2.5 5.0 1.9 Iris-virginica 147 6.5 3.0 5.2 2.0 Iris-virginica 148 6.2 3.4 5.4 2.3 Iris-virginica 149 5.9 3.0 5.1 1.8 Iris-virginica Hitung jaraknya \u00b6 sepal length sepal width petal length petal width class (A-S1)^2 (B-S2)^2 (C-S3)^2 (D-S4)^2 SQRT(DPIS) 0 5.6 2.9 3.6 1.30 Iris-versicolor 0.36 1.69 0.01 0.5329 1.610248 1 5.6 3.0 4.1 1.30 Iris-versicolor 0.36 1.44 0.36 0.5329 1.641006 2 5.1 3.8 1.9 0.40 Iris-setosa 0.01 0.16 2.56 0.0289 1.660994 3 5.7 3.0 4.2 1.20 Iris-versicolor 0.49 1.44 0.49 0.3969 1.678362 4 5.2 2.7 3.9 1.40 Iris-versicolor 0.04 2.25 0.16 0.6889 1.771694 5 5.7 2.9 4.2 1.30 Iris-versicolor 0.49 1.69 0.49 0.5329 1.789665 6 5.7 2.6 3.5 1.00 Iris-versicolor 0.49 2.56 0.00 0.1849 1.798583 7 5.7 2.8 4.1 1.30 Iris-versicolor 0.49 1.96 0.36 0.5329 1.828360 8 4.8 3.4 1.9 0.20 Iris-setosa 0.04 0.64 2.56 0.1369 1.837634 9 5.1 2.5 3.0 1.10 Iris-versicolor 0.01 2.89 0.25 0.2809 1.852269 10 5.8 2.7 4.1 1.00 Iris-versicolor 0.64 2.25 0.36 0.1849 1.853348 11 5.8 2.7 3.9 1.20 Iris-versicolor 0.64 2.25 0.16 0.3969 1.856583 12 5.4 3.0 4.5 1.50 Iris-versicolor 0.16 1.44 1.00 0.8649 1.861424 13 4.9 2.4 3.3 1.00 Iris-versicolor 0.01 3.24 0.04 0.1849 1.864108 14 5.4 3.9 1.7 0.40 Iris-setosa 0.16 0.09 3.24 0.0289 1.875873 15 5.9 3.0 4.2 1.50 Iris-versicolor 0.81 1.44 0.49 0.8649 1.898657 16 5.6 2.7 4.2 1.30 Iris-versicolor 0.36 2.25 0.49 0.5329 1.906017 17 5.6 3.0 4.5 1.50 Iris-versicolor 0.36 1.44 1.00 0.8649 1.914393 18 5.6 2.5 3.9 1.10 Iris-versicolor 0.36 2.89 0.16 0.2809 1.921172 19 6 3.4 4.5 1.60 Iris-versicolor 1.00 0.64 1.00 1.0609 1.923772 20 5.5 2.4 3.7 1.00 Iris-versicolor 0.25 3.24 0.04 0.1849 1.927408 21 5 2.3 3.3 1.00 Iris-versicolor 0.00 3.61 0.04 0.1849 1.958290 22 5.8 2.6 4.0 1.20 Iris-versicolor 0.64 2.56 0.25 0.3969 1.961352 23 5.5 2.4 3.8 1.10 Iris-versicolor 0.25 3.24 0.09 0.2809 1.964917 24 5.1 3.8 1.6 0.20 Iris-setosa 0.01 0.16 3.61 0.1369 1.979116 25 5.5 2.5 4.0 1.30 Iris-versicolor 0.25 2.89 0.25 0.5329 1.980631 26 6.1 2.8 4.0 1.30 Iris-versicolor 1.21 1.96 0.25 0.5329 1.988190 27 5.7 3.8 1.7 0.30 Iris-setosa 0.49 0.16 3.24 0.0729 1.990703 28 5.7 2.8 4.5 1.30 Iris-versicolor 0.49 1.96 1.00 0.5329 1.995720 29 5.5 2.6 4.4 1.20 Iris-versicolor 0.25 2.56 0.81 0.3969 2.004221 ... ... ... ... ... ... ... ... ... ... ... 123 6.3 3.4 5.6 2.40 Iris-virginica 1.69 0.64 4.41 3.3489 3.176303 124 6.7 3.0 5.2 2.30 Iris-virginica 2.89 1.44 2.89 2.9929 3.195763 125 6.9 3.1 5.1 2.30 Iris-virginica 3.61 1.21 2.56 2.9929 3.220699 126 6.4 2.8 5.6 2.10 Iris-virginica 1.96 1.96 4.41 2.3409 3.266634 127 6.9 3.1 5.4 2.10 Iris-virginica 3.61 1.21 3.61 2.3409 3.281905 128 6.7 3.3 5.7 2.10 Iris-virginica 2.89 0.81 4.84 2.3409 3.298621 129 6.4 2.8 5.6 2.20 Iris-virginica 1.96 1.96 4.41 2.6569 3.314649 130 6.8 3.0 5.5 2.10 Iris-virginica 3.24 1.44 4.00 2.3409 3.319774 131 6.5 3.0 5.8 2.20 Iris-virginica 2.25 1.44 5.29 2.6569 3.411290 132 6.7 3.1 5.6 2.40 Iris-virginica 2.89 1.21 4.41 3.3489 3.443675 133 6.7 3.3 5.7 2.50 Iris-virginica 2.89 0.81 4.84 3.7249 3.502128 134 6.9 3.2 5.7 2.30 Iris-virginica 3.61 1.00 4.84 2.9929 3.527450 135 6.3 3.3 6.0 2.50 Iris-virginica 1.69 0.81 6.25 3.7249 3.531982 136 6.7 2.5 5.8 1.80 Iris-virginica 2.89 2.89 5.29 1.5129 3.547238 137 7.2 3.0 5.8 1.60 Iris-virginica 4.84 1.44 5.29 1.0609 3.553998 138 6.8 3.2 5.9 2.30 Iris-virginica 3.24 1.00 5.76 2.9929 3.604567 139 7.2 3.2 6.0 1.80 Iris-virginica 4.84 1.00 6.25 1.5129 3.688211 140 7.1 3.0 5.9 2.10 Iris-virginica 4.41 1.44 5.76 2.3409 3.735090 141 7.2 3.6 6.1 2.50 Iris-virginica 4.84 0.36 6.76 3.7249 3.960417 142 7.4 2.8 6.1 1.90 Iris-virginica 5.76 1.96 6.76 1.7689 4.030992 143 7.3 2.9 6.3 1.80 Iris-virginica 5.29 1.69 7.84 1.5129 4.041398 144 7.7 3.0 6.1 2.30 Iris-virginica 7.29 1.44 6.76 2.9929 4.299174 145 7.9 3.8 6.4 2.00 Iris-virginica 8.41 0.16 8.41 2.0449 4.361754 146 7.6 3.0 6.6 2.10 Iris-virginica 6.76 1.44 9.61 2.3409 4.488975 147 7.7 3.8 6.7 2.20 Iris-virginica 7.29 0.16 10.24 2.6569 4.510754 148 7.7 2.8 6.7 2.00 Iris-virginica 7.29 1.96 10.24 2.0449 4.640571 149 7.7 2.6 6.9 2.30 Iris-virginica 7.29 2.56 11.56 2.9929 4.939929 150 151 Sampel data 152 5 4.2 3.5 0.57 Iris-versicolor Setelah itu sorting dan ambil 5 data teratas \u00b6 sepal length sepal width petal length petal width class 0 5.6 2.9 3.6 1.3 Iris-versicolor 1 5.6 3.0 4.1 1.3 Iris-versicolor 2 5.1 3.8 1.9 0.4 Iris-setosa 3 5.7 3.0 4.2 1.2 Iris-versicolor 4 5.2 2.7 3.9 1.4 Iris-versicolor Lalu hitung berat antar variasi dengan 1/jaraknya : \u00b6 D 1/D Setosa Versicolor Virginica 0 1.610248 0.621022187 0 0.621022 0 1 1.641006 0.609382372 0 0.609382 0 2 1.660994 0.602049251 0.602049251 0.000000 0 3 1.678362 0.595818913 0 0.595819 0 4 1.771694 0.564431522 0 0.564432 0 5 6 SUM 0.602049251 2.390655 0 7 8 CLASS Iris-versicolor Jadi dari perhitungan diatas didapatkan hasil dari sampel adalah class : Iris-versicolor MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"K-NN"},{"location":"knn/#weight-knn","text":"","title":"Weight-KNN"},{"location":"knn/#w-knn-weighted-k-nearest-neighbour","text":"adalah versi modifikasi dari K-NN . Salah satu dari banyak masalah yang mempengaruhi kinerja algoritma kNN adalah pilihan hyperparameter k. Jika k terlalu kecil, algoritme akan lebih sensitif terhadap pencilan. Jika k terlalu besar, maka lingkungan tersebut mungkin memasukkan terlalu banyak poin dari kelas lain. Masalah lainnya adalah pendekatan untuk menggabungkan label kelas. Metode paling sederhana adalah dengan mengambil suara terbanyak, tetapi ini bisa menjadi masalah jika tetangga terdekat sangat bervariasi dalam jarak mereka dan tetangga terdekat lebih andal menunjukkan kelas objek.","title":"W-KNN (Weighted K-Nearest Neighbour)"},{"location":"knn/#algoritma","text":"Misalkan L = {(x i , y i ), i = 1,. . . , n} menjadi seperangkat pelatihan pengamatan x i dengan kelas yang diberikan y i dan membiarkan x menjadi pengamatan baru (titik kueri), yang label kelasnya y harus diprediksi. Hitung d (x i , x) untuk i = 1,. . . , n, jarak antara titik kueri dan setiap titik lainnya dalam set pelatihan. Pilih D '\u2286 D, set k poin data pelatihan terdekat ke poin kueri Memprediksi kelas titik kueri, menggunakan pemungutan suara berbobot jarak. V mewakili label kelas.","title":"Algoritma"},{"location":"knn/#contoh","text":"","title":"Contoh :"},{"location":"knn/#data-iris","text":"sepal length sepal width petal length petal width class 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa 5 5.4 3.9 1.7 0.4 Iris-setosa 6 4.6 3.4 1.4 0.3 Iris-setosa 7 5.0 3.4 1.5 0.2 Iris-setosa 8 4.4 2.9 1.4 0.2 Iris-setosa 9 4.9 3.1 1.5 0.1 Iris-setosa 10 5.4 3.7 1.5 0.2 Iris-setosa 11 4.8 3.4 1.6 0.2 Iris-setosa 12 4.8 3.0 1.4 0.1 Iris-setosa 13 4.3 3.0 1.1 0.1 Iris-setosa 14 5.8 4.0 1.2 0.2 Iris-setosa 15 5.7 4.4 1.5 0.4 Iris-setosa 16 5.4 3.9 1.3 0.4 Iris-setosa 17 5.1 3.5 1.4 0.3 Iris-setosa 18 5.7 3.8 1.7 0.3 Iris-setosa 19 5.1 3.8 1.5 0.3 Iris-setosa 20 5.4 3.4 1.7 0.2 Iris-setosa 21 5.1 3.7 1.5 0.4 Iris-setosa 22 4.6 3.6 1.0 0.2 Iris-setosa 23 5.1 3.3 1.7 0.5 Iris-setosa 24 4.8 3.4 1.9 0.2 Iris-setosa 25 5.0 3.0 1.6 0.2 Iris-setosa 26 5.0 3.4 1.6 0.4 Iris-setosa 27 5.2 3.5 1.5 0.2 Iris-setosa 28 5.2 3.4 1.4 0.2 Iris-setosa 29 4.7 3.2 1.6 0.2 Iris-setosa ... ... ... ... ... ... 120 6.9 3.2 5.7 2.3 Iris-virginica 121 5.6 2.8 4.9 2.0 Iris-virginica 122 7.7 2.8 6.7 2.0 Iris-virginica 123 6.3 2.7 4.9 1.8 Iris-virginica 124 6.7 3.3 5.7 2.1 Iris-virginica 125 7.2 3.2 6.0 1.8 Iris-virginica 126 6.2 2.8 4.8 1.8 Iris-virginica 127 6.1 3.0 4.9 1.8 Iris-virginica 128 6.4 2.8 5.6 2.1 Iris-virginica 129 7.2 3.0 5.8 1.6 Iris-virginica 130 7.4 2.8 6.1 1.9 Iris-virginica 131 7.9 3.8 6.4 2.0 Iris-virginica 132 6.4 2.8 5.6 2.2 Iris-virginica 133 6.3 2.8 5.1 1.5 Iris-virginica 134 6.1 2.6 5.6 1.4 Iris-virginica 135 7.7 3.0 6.1 2.3 Iris-virginica 136 6.3 3.4 5.6 2.4 Iris-virginica 137 6.4 3.1 5.5 1.8 Iris-virginica 138 6.0 3.0 4.8 1.8 Iris-virginica 139 6.9 3.1 5.4 2.1 Iris-virginica 140 6.7 3.1 5.6 2.4 Iris-virginica 141 6.9 3.1 5.1 2.3 Iris-virginica 142 5.8 2.7 5.1 1.9 Iris-virginica 143 6.8 3.2 5.9 2.3 Iris-virginica 144 6.7 3.3 5.7 2.5 Iris-virginica 145 6.7 3.0 5.2 2.3 Iris-virginica 146 6.3 2.5 5.0 1.9 Iris-virginica 147 6.5 3.0 5.2 2.0 Iris-virginica 148 6.2 3.4 5.4 2.3 Iris-virginica 149 5.9 3.0 5.1 1.8 Iris-virginica","title":"Data iris"},{"location":"knn/#hitung-jaraknya","text":"sepal length sepal width petal length petal width class (A-S1)^2 (B-S2)^2 (C-S3)^2 (D-S4)^2 SQRT(DPIS) 0 5.6 2.9 3.6 1.30 Iris-versicolor 0.36 1.69 0.01 0.5329 1.610248 1 5.6 3.0 4.1 1.30 Iris-versicolor 0.36 1.44 0.36 0.5329 1.641006 2 5.1 3.8 1.9 0.40 Iris-setosa 0.01 0.16 2.56 0.0289 1.660994 3 5.7 3.0 4.2 1.20 Iris-versicolor 0.49 1.44 0.49 0.3969 1.678362 4 5.2 2.7 3.9 1.40 Iris-versicolor 0.04 2.25 0.16 0.6889 1.771694 5 5.7 2.9 4.2 1.30 Iris-versicolor 0.49 1.69 0.49 0.5329 1.789665 6 5.7 2.6 3.5 1.00 Iris-versicolor 0.49 2.56 0.00 0.1849 1.798583 7 5.7 2.8 4.1 1.30 Iris-versicolor 0.49 1.96 0.36 0.5329 1.828360 8 4.8 3.4 1.9 0.20 Iris-setosa 0.04 0.64 2.56 0.1369 1.837634 9 5.1 2.5 3.0 1.10 Iris-versicolor 0.01 2.89 0.25 0.2809 1.852269 10 5.8 2.7 4.1 1.00 Iris-versicolor 0.64 2.25 0.36 0.1849 1.853348 11 5.8 2.7 3.9 1.20 Iris-versicolor 0.64 2.25 0.16 0.3969 1.856583 12 5.4 3.0 4.5 1.50 Iris-versicolor 0.16 1.44 1.00 0.8649 1.861424 13 4.9 2.4 3.3 1.00 Iris-versicolor 0.01 3.24 0.04 0.1849 1.864108 14 5.4 3.9 1.7 0.40 Iris-setosa 0.16 0.09 3.24 0.0289 1.875873 15 5.9 3.0 4.2 1.50 Iris-versicolor 0.81 1.44 0.49 0.8649 1.898657 16 5.6 2.7 4.2 1.30 Iris-versicolor 0.36 2.25 0.49 0.5329 1.906017 17 5.6 3.0 4.5 1.50 Iris-versicolor 0.36 1.44 1.00 0.8649 1.914393 18 5.6 2.5 3.9 1.10 Iris-versicolor 0.36 2.89 0.16 0.2809 1.921172 19 6 3.4 4.5 1.60 Iris-versicolor 1.00 0.64 1.00 1.0609 1.923772 20 5.5 2.4 3.7 1.00 Iris-versicolor 0.25 3.24 0.04 0.1849 1.927408 21 5 2.3 3.3 1.00 Iris-versicolor 0.00 3.61 0.04 0.1849 1.958290 22 5.8 2.6 4.0 1.20 Iris-versicolor 0.64 2.56 0.25 0.3969 1.961352 23 5.5 2.4 3.8 1.10 Iris-versicolor 0.25 3.24 0.09 0.2809 1.964917 24 5.1 3.8 1.6 0.20 Iris-setosa 0.01 0.16 3.61 0.1369 1.979116 25 5.5 2.5 4.0 1.30 Iris-versicolor 0.25 2.89 0.25 0.5329 1.980631 26 6.1 2.8 4.0 1.30 Iris-versicolor 1.21 1.96 0.25 0.5329 1.988190 27 5.7 3.8 1.7 0.30 Iris-setosa 0.49 0.16 3.24 0.0729 1.990703 28 5.7 2.8 4.5 1.30 Iris-versicolor 0.49 1.96 1.00 0.5329 1.995720 29 5.5 2.6 4.4 1.20 Iris-versicolor 0.25 2.56 0.81 0.3969 2.004221 ... ... ... ... ... ... ... ... ... ... ... 123 6.3 3.4 5.6 2.40 Iris-virginica 1.69 0.64 4.41 3.3489 3.176303 124 6.7 3.0 5.2 2.30 Iris-virginica 2.89 1.44 2.89 2.9929 3.195763 125 6.9 3.1 5.1 2.30 Iris-virginica 3.61 1.21 2.56 2.9929 3.220699 126 6.4 2.8 5.6 2.10 Iris-virginica 1.96 1.96 4.41 2.3409 3.266634 127 6.9 3.1 5.4 2.10 Iris-virginica 3.61 1.21 3.61 2.3409 3.281905 128 6.7 3.3 5.7 2.10 Iris-virginica 2.89 0.81 4.84 2.3409 3.298621 129 6.4 2.8 5.6 2.20 Iris-virginica 1.96 1.96 4.41 2.6569 3.314649 130 6.8 3.0 5.5 2.10 Iris-virginica 3.24 1.44 4.00 2.3409 3.319774 131 6.5 3.0 5.8 2.20 Iris-virginica 2.25 1.44 5.29 2.6569 3.411290 132 6.7 3.1 5.6 2.40 Iris-virginica 2.89 1.21 4.41 3.3489 3.443675 133 6.7 3.3 5.7 2.50 Iris-virginica 2.89 0.81 4.84 3.7249 3.502128 134 6.9 3.2 5.7 2.30 Iris-virginica 3.61 1.00 4.84 2.9929 3.527450 135 6.3 3.3 6.0 2.50 Iris-virginica 1.69 0.81 6.25 3.7249 3.531982 136 6.7 2.5 5.8 1.80 Iris-virginica 2.89 2.89 5.29 1.5129 3.547238 137 7.2 3.0 5.8 1.60 Iris-virginica 4.84 1.44 5.29 1.0609 3.553998 138 6.8 3.2 5.9 2.30 Iris-virginica 3.24 1.00 5.76 2.9929 3.604567 139 7.2 3.2 6.0 1.80 Iris-virginica 4.84 1.00 6.25 1.5129 3.688211 140 7.1 3.0 5.9 2.10 Iris-virginica 4.41 1.44 5.76 2.3409 3.735090 141 7.2 3.6 6.1 2.50 Iris-virginica 4.84 0.36 6.76 3.7249 3.960417 142 7.4 2.8 6.1 1.90 Iris-virginica 5.76 1.96 6.76 1.7689 4.030992 143 7.3 2.9 6.3 1.80 Iris-virginica 5.29 1.69 7.84 1.5129 4.041398 144 7.7 3.0 6.1 2.30 Iris-virginica 7.29 1.44 6.76 2.9929 4.299174 145 7.9 3.8 6.4 2.00 Iris-virginica 8.41 0.16 8.41 2.0449 4.361754 146 7.6 3.0 6.6 2.10 Iris-virginica 6.76 1.44 9.61 2.3409 4.488975 147 7.7 3.8 6.7 2.20 Iris-virginica 7.29 0.16 10.24 2.6569 4.510754 148 7.7 2.8 6.7 2.00 Iris-virginica 7.29 1.96 10.24 2.0449 4.640571 149 7.7 2.6 6.9 2.30 Iris-virginica 7.29 2.56 11.56 2.9929 4.939929 150 151 Sampel data 152 5 4.2 3.5 0.57 Iris-versicolor","title":"Hitung jaraknya"},{"location":"knn/#setelah-itu-sorting-dan-ambil-5-data-teratas","text":"sepal length sepal width petal length petal width class 0 5.6 2.9 3.6 1.3 Iris-versicolor 1 5.6 3.0 4.1 1.3 Iris-versicolor 2 5.1 3.8 1.9 0.4 Iris-setosa 3 5.7 3.0 4.2 1.2 Iris-versicolor 4 5.2 2.7 3.9 1.4 Iris-versicolor","title":"Setelah itu sorting dan ambil 5 data teratas"},{"location":"knn/#lalu-hitung-berat-antar-variasi-dengan-1jaraknya","text":"D 1/D Setosa Versicolor Virginica 0 1.610248 0.621022187 0 0.621022 0 1 1.641006 0.609382372 0 0.609382 0 2 1.660994 0.602049251 0.602049251 0.000000 0 3 1.678362 0.595818913 0 0.595819 0 4 1.771694 0.564431522 0 0.564432 0 5 6 SUM 0.602049251 2.390655 0 7 8 CLASS Iris-versicolor Jadi dari perhitungan diatas didapatkan hasil dari sampel adalah class : Iris-versicolor MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Lalu hitung berat antar variasi dengan 1/jaraknya :"},{"location":"na%C3%AFvebayes/","text":"Na\u00efve Bayes \u00b6 Na\u00efve Bayes merupakan sebuah pengklasifikasian probabilistik sederhana yang menghitung sekumpulan probabilitas dengan menjumlahkan frekuensi dan kombinasi nilai dari dataset yang diberikan. Algoritma mengunakan teorema Bayes dan mengasumsikan semua atribut independen atau tidak saling ketergantungan yang diberikan oleh nilai pada variabel kelas. Definisi lain mengatakan Na\u00efve Bayes merupakan pengklasifikasian dengan metode probabilitas dan statistik yang dikemukan oleh ilmuwan Inggris Thomas Bayes, yaitu memprediksi peluang di masa depan berdasarkan pengalaman di masa sebelumnya. Na\u00efve Bayes didasarkan pada asumsi penyederhanaan bahwa nilai atribut secara kondisional saling bebas jika diberikan nilai output. Dengan kata lain, diberikan nilai output, probabilitas mengamati secara bersama adalah produk dari probabilitas individu. Keuntungan penggunaan Naive Bayes adalah bahwa metode ini hanya membutuhkan jumlah data pelatihan (Training Data) yang kecil untuk menentukan estimasi paremeter yang diperlukan dalam proses pengklasifikasian. Naive Bayes sering bekerja jauh lebih baik dalam kebanyakan situasi dunia nyata yang kompleks dari pada yang diharapkan. Naive Bayes Classifier dinilai bekerja sangat baik dibanding dengan model classifier lainnya, yaitu Na\u00efve Bayes Classifier memiliki tingkat akurasi yg lebih baik dibanding model classifier lainnya. Pre-requisites \u00b6 Pemahaman terhadap dasar-dasar Statistika terutama mengenai Probabilitas/Peluang Pemahaman terhadap dasar-dasar teknologi Web,HTML dan CSS Pemahaman terhadap dasar-dasar basis data/database, terutama query SQL pada MySQL/mariaDB Pemahaman terhadap dasar-dasar pemrograman PHP, terutama fungsi-fungsi koneksi database dan pengelolaan tipe data array Teorema Na\u00efve Bayes \u00b6 Sebelum menjelaskan Na\u00efve Bayes Classifier ini, akan dijelaskan terlebih dahulu Teorema Bayes yang menjadi dasar dari metoda tersebut. Pada Teorema Bayes , bila terdapat dua kejadian yang terpisah (misalkan X dan H ), maka Teorema Bayes dirumuskan sebagai berikut.: $$ P(H|X) = \\frac{(P(X|H)P(H))}{P(X)} $$ Keterangan Teorema Bayes sering pula dikembangkan mengingat berlakunya hukum probabilitas total, menjadi seperti berikut: $$ P(\\operatorname{C_i|D}) = \\frac{\\left( P(D|C_i)\\right)P(C_i)}{P(D)} $$ Keterangan Untuk menjelaskan Teorema Na\u00efve Bayes , perlu diketahui bahwa proses klasifikasi memerlukan sejumlah petunjuk untuk menentukan kelas apa yang cocok bagi sampel yang dianalisis tersebut. Karena itu, Teorema Bayes di atas disesuaikan sebagai berikut: Di mana Variabel C merepresentasikan kelas, sementara variabel F1 ... Fn merepresentasikan karakteristik petunjuk yang dibutuhkan untuk melakukan klasifikasi. Maka rumus tersebut menjelaskan bahwa peluang masuknya sampel karakteristik tertentu dalam kelas C ( Posterior ) adalah peluang munculnya kelas C (sebelum masuknya sampel tersebut, seringkali disebut prior ), dikali dengan peluang kemunculan karakteristik-karakteristik sampel pada kelas C (disebut juga likelihood ), dibagi dengan peluang kemunculan karakteristik-karakteristik sampel secara global (disebut juga evidence ). Naive Bayes Classifier atau bisa di sebut sebagai multinomina naive bayes merupakan model penyederhanaan dari algoritma bayes yang cocok dalam pengklasifikasian text atau dokumen. P adalah probabilitas yang muncul. Untuk data numerik P adalah: $$ P(x=v|C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_k}}\\exp\\left(-\\frac{(v-\\mu_k)^2}{2\\sigma^2_k}\\right) $$ Contoh Program python iris = datasets.load_iris() data = [list(s)+[iris.target_names[iris.target[i]]] for i,s in enumerate(iris.data)] dataset = DataFrame(data, columns=iris.feature_names+['class']).sample(frac=0.2) table(dataset) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 7.7 2.6 6.9 2.3 virginica 5.4 3.7 1.5 0.2 setosa 6.9 3.1 5.4 2.1 virginica 6.6 3 4.4 1.4 versicolor 4.6 3.1 1.5 0.2 setosa 5.1 3.4 1.5 0.2 setosa 7.7 3.8 6.7 2.2 virginica 6 2.2 4 1 versicolor 7.2 3.2 6 1.8 virginica 5.9 3.2 4.8 1.8 versicolor 6.3 2.7 4.9 1.8 virginica 6.3 3.3 4.7 1.6 versicolor 6.2 3.4 5.4 2.3 virginica 5.4 3.4 1.5 0.4 setosa 5.2 3.5 1.5 0.2 setosa 4.4 3.2 1.3 0.2 setosa 6.1 2.6 5.6 1.4 virginica 5.1 3.8 1.9 0.4 setosa 5.7 3 4.2 1.2 versicolor 5.4 3.4 1.7 0.2 setosa 4.9 3.1 1.5 0.2 setosa 6 3.4 4.5 1.6 versicolor 6.1 2.8 4.7 1.2 versicolor 5.8 2.7 5.1 1.9 virginica 4.8 3.4 1.6 0.2 setosa 6.1 3 4.9 1.8 virginica 6.5 3 5.8 2.2 virginica 5.7 2.9 4.2 1.3 versicolor 6.8 2.8 4.8 1.4 versicolor 5.1 3.3 1.7 0.5 setosa Sampel data untuk di tes \u00b6 \u00b6 python test = [3,5,2,4] print(\"sampel data: \", test) sampel data: [3, 5, 2, 4] Identifikasi Per Grup Class Target untuk data \u00b6 \u00b6 python dataset_classes = {} # table per classes for key,group in dataset.groupby('class'): mu_s = [group[c].mean() for c in group.columns[:-1]] sigma_s = [group[c].std() for c in group.columns[:-1]] dataset_classes[key] = [group, mu_s, sigma_s] print(key, \"===>\") print('Mu_s =>', array(mu_s)) print('Sigma_s =>', array(sigma_s)) table(group) setosa ===> Mu_s => [5.03636364 3.39090909 1.56363636 0.26363636] Sigma_s => [0.33248377 0.221154 0.15666989 0.11200649] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.4 3.7 1.5 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.1 3.4 1.5 0.2 setosa 5.4 3.4 1.5 0.4 setosa 5.2 3.5 1.5 0.2 setosa 4.4 3.2 1.3 0.2 setosa 5.1 3.8 1.9 0.4 setosa 5.4 3.4 1.7 0.2 setosa 4.9 3.1 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 5.1 3.3 1.7 0.5 setosa versicolor ===> Mu_s => [6.12222222 2.95555556 4.47777778 1.38888889] Sigma_s => [0.38005848 0.35394601 0.29486343 0.24720662] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 6.6 3 4.4 1.4 versicolor 6 2.2 4 1 versicolor 5.9 3.2 4.8 1.8 versicolor 6.3 3.3 4.7 1.6 versicolor 5.7 3 4.2 1.2 versicolor 6 3.4 4.5 1.6 versicolor 6.1 2.8 4.7 1.2 versicolor 5.7 2.9 4.2 1.3 versicolor 6.8 2.8 4.8 1.4 versicolor virginica ===> Mu_s => [6.65 3.01 5.67 1.98] Sigma_s => [0.68677993 0.38715486 0.69610025 0.28982753] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 7.7 2.6 6.9 2.3 virginica 6.9 3.1 5.4 2.1 virginica 7.7 3.8 6.7 2.2 virginica 7.2 3.2 6 1.8 virginica 6.3 2.7 4.9 1.8 virginica 6.2 3.4 5.4 2.3 virginica 6.1 2.6 5.6 1.4 virginica 5.8 2.7 5.1 1.9 virginica 6.1 3 4.9 1.8 virginica 6.5 3 5.8 2.2 virginica Hitung Probabilitas Prior dan Likehood \u00b6 `python def numericalPriorProbability(v, mu, sigma): return (1.0/sqrt(2 * pi * (sigma 2))*exp(-((v-mu) 2)/(2 (sigma *2)))) def categoricalProbability(sample,universe): return sample.shape[0]/universe.shape[0] Ps = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(test)]+ [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()]) table(DataFrame(Ps, columns=[\"classes\"]+[\"P( %d | C )\" % d for d in test]+[\"P( C )\"])) ` classes P( 3 | C ) P( 5 | C ) P( 2 | C ) P( 4 | C ) P( C ) setosa 8.58066e-09 5.76458e-12 0.0526488 8.1872e-242 0.366667 versicolor 2.32391e-15 6.41354e-08 6.27963e-16 9.58684e-25 0.3 virginica 4.27211e-07 1.88775e-06 5.27627e-07 3.89569e-11 0.333333 Kesimpulan \u00b6 \u00b6 python Pss = ([[r[0], prod(r[1:])] for r in Ps]) PDss = DataFrame(Pss, columns=['class', 'probability']).sort_values('probability')[::-1] table(PDss) class probability virginica 5.52556e-30 versicolor 2.69183e-62 setosa 7.81778e-263 python print(\"Prediksi Bayes untuk\", test, \"adalah\", PDss.values[0,0]) Prediksi Bayes untuk [3, 5, 2, 4] adalah virginica ## Naive Bayes untuk Data Iris `python def predict(sampel): priorLikehoods = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(sampel)]+ [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()]) products = ([[r[0], prod(r[1:])] for r in priorLikehoods]) result = DataFrame(products, columns=['class', 'probability']).sort_values('probability')[::-1] return result.values[0,0] dataset_test = DataFrame([list(d)+[predict(d[:4])] for d in data], columns=list(dataset.columns)+['predicted class (by predict())']) table(dataset_test) ` sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class predicted class (by predict()) 5.1 3.5 1.4 0.2 setosa setosa 4.9 3 1.4 0.2 setosa setosa 4.7 3.2 1.3 0.2 setosa setosa 4.6 3.1 1.5 0.2 setosa setosa 5 3.6 1.4 0.2 setosa setosa 5.4 3.9 1.7 0.4 setosa setosa 4.6 3.4 1.4 0.3 setosa setosa 5 3.4 1.5 0.2 setosa setosa 4.4 2.9 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.1 setosa setosa 5.4 3.7 1.5 0.2 setosa setosa 4.8 3.4 1.6 0.2 setosa setosa 4.8 3 1.4 0.1 setosa setosa 4.3 3 1.1 0.1 setosa setosa 5.8 4 1.2 0.2 setosa setosa 5.7 4.4 1.5 0.4 setosa setosa 5.4 3.9 1.3 0.4 setosa setosa 5.1 3.5 1.4 0.3 setosa setosa 5.7 3.8 1.7 0.3 setosa setosa 5.1 3.8 1.5 0.3 setosa setosa 5.4 3.4 1.7 0.2 setosa setosa 5.1 3.7 1.5 0.4 setosa setosa 4.6 3.6 1 0.2 setosa setosa 5.1 3.3 1.7 0.5 setosa setosa 4.8 3.4 1.9 0.2 setosa setosa 5 3 1.6 0.2 setosa setosa 5 3.4 1.6 0.4 setosa setosa 5.2 3.5 1.5 0.2 setosa setosa 5.2 3.4 1.4 0.2 setosa setosa 4.7 3.2 1.6 0.2 setosa setosa 4.8 3.1 1.6 0.2 setosa setosa 5.4 3.4 1.5 0.4 setosa setosa 5.2 4.1 1.5 0.1 setosa setosa 5.5 4.2 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.2 setosa setosa 5 3.2 1.2 0.2 setosa setosa 5.5 3.5 1.3 0.2 setosa setosa 4.9 3.6 1.4 0.1 setosa setosa 4.4 3 1.3 0.2 setosa setosa 5.1 3.4 1.5 0.2 setosa setosa 5 3.5 1.3 0.3 setosa setosa 4.5 2.3 1.3 0.3 setosa setosa 4.4 3.2 1.3 0.2 setosa setosa 5 3.5 1.6 0.6 setosa setosa 5.1 3.8 1.9 0.4 setosa setosa 4.8 3 1.4 0.3 setosa setosa 5.1 3.8 1.6 0.2 setosa setosa 4.6 3.2 1.4 0.2 setosa setosa 5.3 3.7 1.5 0.2 setosa setosa 5 3.3 1.4 0.2 setosa setosa 7 3.2 4.7 1.4 versicolor versicolor 6.4 3.2 4.5 1.5 versicolor versicolor 6.9 3.1 4.9 1.5 versicolor versicolor 5.5 2.3 4 1.3 versicolor versicolor 6.5 2.8 4.6 1.5 versicolor versicolor 5.7 2.8 4.5 1.3 versicolor versicolor 6.3 3.3 4.7 1.6 versicolor versicolor 4.9 2.4 3.3 1 versicolor versicolor 6.6 2.9 4.6 1.3 versicolor versicolor 5.2 2.7 3.9 1.4 versicolor versicolor 5 2 3.5 1 versicolor versicolor 5.9 3 4.2 1.5 versicolor versicolor 6 2.2 4 1 versicolor versicolor 6.1 2.9 4.7 1.4 versicolor versicolor 5.6 2.9 3.6 1.3 versicolor versicolor 6.7 3.1 4.4 1.4 versicolor versicolor 5.6 3 4.5 1.5 versicolor versicolor 5.8 2.7 4.1 1 versicolor versicolor 6.2 2.2 4.5 1.5 versicolor versicolor 5.6 2.5 3.9 1.1 versicolor versicolor 5.9 3.2 4.8 1.8 versicolor versicolor 6.1 2.8 4 1.3 versicolor versicolor 6.3 2.5 4.9 1.5 versicolor versicolor 6.1 2.8 4.7 1.2 versicolor versicolor 6.4 2.9 4.3 1.3 versicolor versicolor 6.6 3 4.4 1.4 versicolor versicolor 6.8 2.8 4.8 1.4 versicolor versicolor 6.7 3 5 1.7 versicolor virginica 6 2.9 4.5 1.5 versicolor versicolor 5.7 2.6 3.5 1 versicolor versicolor 5.5 2.4 3.8 1.1 versicolor versicolor 5.5 2.4 3.7 1 versicolor versicolor 5.8 2.7 3.9 1.2 versicolor versicolor 6 2.7 5.1 1.6 versicolor versicolor 5.4 3 4.5 1.5 versicolor versicolor 6 3.4 4.5 1.6 versicolor versicolor 6.7 3.1 4.7 1.5 versicolor versicolor 6.3 2.3 4.4 1.3 versicolor versicolor 5.6 3 4.1 1.3 versicolor versicolor 5.5 2.5 4 1.3 versicolor versicolor 5.5 2.6 4.4 1.2 versicolor versicolor 6.1 3 4.6 1.4 versicolor versicolor 5.8 2.6 4 1.2 versicolor versicolor 5 2.3 3.3 1 versicolor versicolor 5.6 2.7 4.2 1.3 versicolor versicolor 5.7 3 4.2 1.2 versicolor versicolor 5.7 2.9 4.2 1.3 versicolor versicolor 6.2 2.9 4.3 1.3 versicolor versicolor 5.1 2.5 3 1.1 versicolor virginica 5.7 2.8 4.1 1.3 versicolor versicolor 6.3 3.3 6 2.5 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 7.1 3 5.9 2.1 virginica virginica 6.3 2.9 5.6 1.8 virginica virginica 6.5 3 5.8 2.2 virginica virginica 7.6 3 6.6 2.1 virginica virginica 4.9 2.5 4.5 1.7 virginica versicolor 7.3 2.9 6.3 1.8 virginica virginica 6.7 2.5 5.8 1.8 virginica virginica 7.2 3.6 6.1 2.5 virginica virginica 6.5 3.2 5.1 2 virginica virginica 6.4 2.7 5.3 1.9 virginica virginica 6.8 3 5.5 2.1 virginica virginica 5.7 2.5 5 2 virginica virginica 5.8 2.8 5.1 2.4 virginica virginica 6.4 3.2 5.3 2.3 virginica virginica 6.5 3 5.5 1.8 virginica virginica 7.7 3.8 6.7 2.2 virginica virginica 7.7 2.6 6.9 2.3 virginica virginica 6 2.2 5 1.5 virginica versicolor 6.9 3.2 5.7 2.3 virginica virginica 5.6 2.8 4.9 2 virginica virginica 7.7 2.8 6.7 2 virginica virginica 6.3 2.7 4.9 1.8 virginica versicolor 6.7 3.3 5.7 2.1 virginica virginica 7.2 3.2 6 1.8 virginica virginica 6.2 2.8 4.8 1.8 virginica versicolor 6.1 3 4.9 1.8 virginica versicolor 6.4 2.8 5.6 2.1 virginica virginica 7.2 3 5.8 1.6 virginica virginica 7.4 2.8 6.1 1.9 virginica virginica 7.9 3.8 6.4 2 virginica virginica 6.4 2.8 5.6 2.2 virginica virginica 6.3 2.8 5.1 1.5 virginica versicolor 6.1 2.6 5.6 1.4 virginica virginica 7.7 3 6.1 2.3 virginica virginica 6.3 3.4 5.6 2.4 virginica virginica 6.4 3.1 5.5 1.8 virginica virginica 6 3 4.8 1.8 virginica versicolor 6.9 3.1 5.4 2.1 virginica virginica 6.7 3.1 5.6 2.4 virginica virginica 6.9 3.1 5.1 2.3 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 6.8 3.2 5.9 2.3 virginica virginica 6.7 3.3 5.7 2.5 virginica virginica 6.7 3 5.2 2.3 virginica virginica 6.3 2.5 5 1.9 virginica virginica 6.5 3 5.2 2 virginica virginica 6.2 3.4 5.4 2.3 virginica virginica 5.9 3 5.1 1.8 virginica virginica python corrects = dataset_test.loc[dataset_test['class'] == dataset_test['predicted class (by predict())']].shape[0] print('Prediksi Training Bayes: %d of %d == %f %%' % (corrects, len(data), corrects / len(data) * 100)) Prediksi Training Bayes: 141 of 150 == 94.000000 % MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Na\u00efve bayes"},{"location":"na%C3%AFvebayes/#naive-bayes","text":"Na\u00efve Bayes merupakan sebuah pengklasifikasian probabilistik sederhana yang menghitung sekumpulan probabilitas dengan menjumlahkan frekuensi dan kombinasi nilai dari dataset yang diberikan. Algoritma mengunakan teorema Bayes dan mengasumsikan semua atribut independen atau tidak saling ketergantungan yang diberikan oleh nilai pada variabel kelas. Definisi lain mengatakan Na\u00efve Bayes merupakan pengklasifikasian dengan metode probabilitas dan statistik yang dikemukan oleh ilmuwan Inggris Thomas Bayes, yaitu memprediksi peluang di masa depan berdasarkan pengalaman di masa sebelumnya. Na\u00efve Bayes didasarkan pada asumsi penyederhanaan bahwa nilai atribut secara kondisional saling bebas jika diberikan nilai output. Dengan kata lain, diberikan nilai output, probabilitas mengamati secara bersama adalah produk dari probabilitas individu. Keuntungan penggunaan Naive Bayes adalah bahwa metode ini hanya membutuhkan jumlah data pelatihan (Training Data) yang kecil untuk menentukan estimasi paremeter yang diperlukan dalam proses pengklasifikasian. Naive Bayes sering bekerja jauh lebih baik dalam kebanyakan situasi dunia nyata yang kompleks dari pada yang diharapkan. Naive Bayes Classifier dinilai bekerja sangat baik dibanding dengan model classifier lainnya, yaitu Na\u00efve Bayes Classifier memiliki tingkat akurasi yg lebih baik dibanding model classifier lainnya.","title":"Na\u00efve Bayes"},{"location":"na%C3%AFvebayes/#pre-requisites","text":"Pemahaman terhadap dasar-dasar Statistika terutama mengenai Probabilitas/Peluang Pemahaman terhadap dasar-dasar teknologi Web,HTML dan CSS Pemahaman terhadap dasar-dasar basis data/database, terutama query SQL pada MySQL/mariaDB Pemahaman terhadap dasar-dasar pemrograman PHP, terutama fungsi-fungsi koneksi database dan pengelolaan tipe data array","title":"Pre-requisites"},{"location":"na%C3%AFvebayes/#teorema-naive-bayes","text":"Sebelum menjelaskan Na\u00efve Bayes Classifier ini, akan dijelaskan terlebih dahulu Teorema Bayes yang menjadi dasar dari metoda tersebut. Pada Teorema Bayes , bila terdapat dua kejadian yang terpisah (misalkan X dan H ), maka Teorema Bayes dirumuskan sebagai berikut.: $$ P(H|X) = \\frac{(P(X|H)P(H))}{P(X)} $$ Keterangan Teorema Bayes sering pula dikembangkan mengingat berlakunya hukum probabilitas total, menjadi seperti berikut: $$ P(\\operatorname{C_i|D}) = \\frac{\\left( P(D|C_i)\\right)P(C_i)}{P(D)} $$ Keterangan Untuk menjelaskan Teorema Na\u00efve Bayes , perlu diketahui bahwa proses klasifikasi memerlukan sejumlah petunjuk untuk menentukan kelas apa yang cocok bagi sampel yang dianalisis tersebut. Karena itu, Teorema Bayes di atas disesuaikan sebagai berikut: Di mana Variabel C merepresentasikan kelas, sementara variabel F1 ... Fn merepresentasikan karakteristik petunjuk yang dibutuhkan untuk melakukan klasifikasi. Maka rumus tersebut menjelaskan bahwa peluang masuknya sampel karakteristik tertentu dalam kelas C ( Posterior ) adalah peluang munculnya kelas C (sebelum masuknya sampel tersebut, seringkali disebut prior ), dikali dengan peluang kemunculan karakteristik-karakteristik sampel pada kelas C (disebut juga likelihood ), dibagi dengan peluang kemunculan karakteristik-karakteristik sampel secara global (disebut juga evidence ). Naive Bayes Classifier atau bisa di sebut sebagai multinomina naive bayes merupakan model penyederhanaan dari algoritma bayes yang cocok dalam pengklasifikasian text atau dokumen. P adalah probabilitas yang muncul. Untuk data numerik P adalah: $$ P(x=v|C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_k}}\\exp\\left(-\\frac{(v-\\mu_k)^2}{2\\sigma^2_k}\\right) $$ Contoh Program python iris = datasets.load_iris() data = [list(s)+[iris.target_names[iris.target[i]]] for i,s in enumerate(iris.data)] dataset = DataFrame(data, columns=iris.feature_names+['class']).sample(frac=0.2) table(dataset) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 7.7 2.6 6.9 2.3 virginica 5.4 3.7 1.5 0.2 setosa 6.9 3.1 5.4 2.1 virginica 6.6 3 4.4 1.4 versicolor 4.6 3.1 1.5 0.2 setosa 5.1 3.4 1.5 0.2 setosa 7.7 3.8 6.7 2.2 virginica 6 2.2 4 1 versicolor 7.2 3.2 6 1.8 virginica 5.9 3.2 4.8 1.8 versicolor 6.3 2.7 4.9 1.8 virginica 6.3 3.3 4.7 1.6 versicolor 6.2 3.4 5.4 2.3 virginica 5.4 3.4 1.5 0.4 setosa 5.2 3.5 1.5 0.2 setosa 4.4 3.2 1.3 0.2 setosa 6.1 2.6 5.6 1.4 virginica 5.1 3.8 1.9 0.4 setosa 5.7 3 4.2 1.2 versicolor 5.4 3.4 1.7 0.2 setosa 4.9 3.1 1.5 0.2 setosa 6 3.4 4.5 1.6 versicolor 6.1 2.8 4.7 1.2 versicolor 5.8 2.7 5.1 1.9 virginica 4.8 3.4 1.6 0.2 setosa 6.1 3 4.9 1.8 virginica 6.5 3 5.8 2.2 virginica 5.7 2.9 4.2 1.3 versicolor 6.8 2.8 4.8 1.4 versicolor 5.1 3.3 1.7 0.5 setosa","title":"Teorema Na\u00efve Bayes"},{"location":"na%C3%AFvebayes/#sampel-data-untuk-di-tes","text":"python test = [3,5,2,4] print(\"sampel data: \", test) sampel data: [3, 5, 2, 4]","title":"Sampel data untuk di tes\u00b6"},{"location":"na%C3%AFvebayes/#identifikasi-per-grup-class-target-untuk-data","text":"python dataset_classes = {} # table per classes for key,group in dataset.groupby('class'): mu_s = [group[c].mean() for c in group.columns[:-1]] sigma_s = [group[c].std() for c in group.columns[:-1]] dataset_classes[key] = [group, mu_s, sigma_s] print(key, \"===>\") print('Mu_s =>', array(mu_s)) print('Sigma_s =>', array(sigma_s)) table(group) setosa ===> Mu_s => [5.03636364 3.39090909 1.56363636 0.26363636] Sigma_s => [0.33248377 0.221154 0.15666989 0.11200649] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.4 3.7 1.5 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.1 3.4 1.5 0.2 setosa 5.4 3.4 1.5 0.4 setosa 5.2 3.5 1.5 0.2 setosa 4.4 3.2 1.3 0.2 setosa 5.1 3.8 1.9 0.4 setosa 5.4 3.4 1.7 0.2 setosa 4.9 3.1 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 5.1 3.3 1.7 0.5 setosa versicolor ===> Mu_s => [6.12222222 2.95555556 4.47777778 1.38888889] Sigma_s => [0.38005848 0.35394601 0.29486343 0.24720662] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 6.6 3 4.4 1.4 versicolor 6 2.2 4 1 versicolor 5.9 3.2 4.8 1.8 versicolor 6.3 3.3 4.7 1.6 versicolor 5.7 3 4.2 1.2 versicolor 6 3.4 4.5 1.6 versicolor 6.1 2.8 4.7 1.2 versicolor 5.7 2.9 4.2 1.3 versicolor 6.8 2.8 4.8 1.4 versicolor virginica ===> Mu_s => [6.65 3.01 5.67 1.98] Sigma_s => [0.68677993 0.38715486 0.69610025 0.28982753] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 7.7 2.6 6.9 2.3 virginica 6.9 3.1 5.4 2.1 virginica 7.7 3.8 6.7 2.2 virginica 7.2 3.2 6 1.8 virginica 6.3 2.7 4.9 1.8 virginica 6.2 3.4 5.4 2.3 virginica 6.1 2.6 5.6 1.4 virginica 5.8 2.7 5.1 1.9 virginica 6.1 3 4.9 1.8 virginica 6.5 3 5.8 2.2 virginica","title":"Identifikasi Per Grup Class Target untuk data\u00b6"},{"location":"na%C3%AFvebayes/#hitung-probabilitas-prior-dan-likehood","text":"`python def numericalPriorProbability(v, mu, sigma): return (1.0/sqrt(2 * pi * (sigma 2))*exp(-((v-mu) 2)/(2 (sigma *2)))) def categoricalProbability(sample,universe): return sample.shape[0]/universe.shape[0] Ps = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(test)]+ [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()]) table(DataFrame(Ps, columns=[\"classes\"]+[\"P( %d | C )\" % d for d in test]+[\"P( C )\"])) ` classes P( 3 | C ) P( 5 | C ) P( 2 | C ) P( 4 | C ) P( C ) setosa 8.58066e-09 5.76458e-12 0.0526488 8.1872e-242 0.366667 versicolor 2.32391e-15 6.41354e-08 6.27963e-16 9.58684e-25 0.3 virginica 4.27211e-07 1.88775e-06 5.27627e-07 3.89569e-11 0.333333","title":"Hitung Probabilitas Prior dan Likehood"},{"location":"na%C3%AFvebayes/#kesimpulan","text":"python Pss = ([[r[0], prod(r[1:])] for r in Ps]) PDss = DataFrame(Pss, columns=['class', 'probability']).sort_values('probability')[::-1] table(PDss) class probability virginica 5.52556e-30 versicolor 2.69183e-62 setosa 7.81778e-263 python print(\"Prediksi Bayes untuk\", test, \"adalah\", PDss.values[0,0]) Prediksi Bayes untuk [3, 5, 2, 4] adalah virginica ## Naive Bayes untuk Data Iris `python def predict(sampel): priorLikehoods = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(sampel)]+ [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()]) products = ([[r[0], prod(r[1:])] for r in priorLikehoods]) result = DataFrame(products, columns=['class', 'probability']).sort_values('probability')[::-1] return result.values[0,0] dataset_test = DataFrame([list(d)+[predict(d[:4])] for d in data], columns=list(dataset.columns)+['predicted class (by predict())']) table(dataset_test) ` sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class predicted class (by predict()) 5.1 3.5 1.4 0.2 setosa setosa 4.9 3 1.4 0.2 setosa setosa 4.7 3.2 1.3 0.2 setosa setosa 4.6 3.1 1.5 0.2 setosa setosa 5 3.6 1.4 0.2 setosa setosa 5.4 3.9 1.7 0.4 setosa setosa 4.6 3.4 1.4 0.3 setosa setosa 5 3.4 1.5 0.2 setosa setosa 4.4 2.9 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.1 setosa setosa 5.4 3.7 1.5 0.2 setosa setosa 4.8 3.4 1.6 0.2 setosa setosa 4.8 3 1.4 0.1 setosa setosa 4.3 3 1.1 0.1 setosa setosa 5.8 4 1.2 0.2 setosa setosa 5.7 4.4 1.5 0.4 setosa setosa 5.4 3.9 1.3 0.4 setosa setosa 5.1 3.5 1.4 0.3 setosa setosa 5.7 3.8 1.7 0.3 setosa setosa 5.1 3.8 1.5 0.3 setosa setosa 5.4 3.4 1.7 0.2 setosa setosa 5.1 3.7 1.5 0.4 setosa setosa 4.6 3.6 1 0.2 setosa setosa 5.1 3.3 1.7 0.5 setosa setosa 4.8 3.4 1.9 0.2 setosa setosa 5 3 1.6 0.2 setosa setosa 5 3.4 1.6 0.4 setosa setosa 5.2 3.5 1.5 0.2 setosa setosa 5.2 3.4 1.4 0.2 setosa setosa 4.7 3.2 1.6 0.2 setosa setosa 4.8 3.1 1.6 0.2 setosa setosa 5.4 3.4 1.5 0.4 setosa setosa 5.2 4.1 1.5 0.1 setosa setosa 5.5 4.2 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.2 setosa setosa 5 3.2 1.2 0.2 setosa setosa 5.5 3.5 1.3 0.2 setosa setosa 4.9 3.6 1.4 0.1 setosa setosa 4.4 3 1.3 0.2 setosa setosa 5.1 3.4 1.5 0.2 setosa setosa 5 3.5 1.3 0.3 setosa setosa 4.5 2.3 1.3 0.3 setosa setosa 4.4 3.2 1.3 0.2 setosa setosa 5 3.5 1.6 0.6 setosa setosa 5.1 3.8 1.9 0.4 setosa setosa 4.8 3 1.4 0.3 setosa setosa 5.1 3.8 1.6 0.2 setosa setosa 4.6 3.2 1.4 0.2 setosa setosa 5.3 3.7 1.5 0.2 setosa setosa 5 3.3 1.4 0.2 setosa setosa 7 3.2 4.7 1.4 versicolor versicolor 6.4 3.2 4.5 1.5 versicolor versicolor 6.9 3.1 4.9 1.5 versicolor versicolor 5.5 2.3 4 1.3 versicolor versicolor 6.5 2.8 4.6 1.5 versicolor versicolor 5.7 2.8 4.5 1.3 versicolor versicolor 6.3 3.3 4.7 1.6 versicolor versicolor 4.9 2.4 3.3 1 versicolor versicolor 6.6 2.9 4.6 1.3 versicolor versicolor 5.2 2.7 3.9 1.4 versicolor versicolor 5 2 3.5 1 versicolor versicolor 5.9 3 4.2 1.5 versicolor versicolor 6 2.2 4 1 versicolor versicolor 6.1 2.9 4.7 1.4 versicolor versicolor 5.6 2.9 3.6 1.3 versicolor versicolor 6.7 3.1 4.4 1.4 versicolor versicolor 5.6 3 4.5 1.5 versicolor versicolor 5.8 2.7 4.1 1 versicolor versicolor 6.2 2.2 4.5 1.5 versicolor versicolor 5.6 2.5 3.9 1.1 versicolor versicolor 5.9 3.2 4.8 1.8 versicolor versicolor 6.1 2.8 4 1.3 versicolor versicolor 6.3 2.5 4.9 1.5 versicolor versicolor 6.1 2.8 4.7 1.2 versicolor versicolor 6.4 2.9 4.3 1.3 versicolor versicolor 6.6 3 4.4 1.4 versicolor versicolor 6.8 2.8 4.8 1.4 versicolor versicolor 6.7 3 5 1.7 versicolor virginica 6 2.9 4.5 1.5 versicolor versicolor 5.7 2.6 3.5 1 versicolor versicolor 5.5 2.4 3.8 1.1 versicolor versicolor 5.5 2.4 3.7 1 versicolor versicolor 5.8 2.7 3.9 1.2 versicolor versicolor 6 2.7 5.1 1.6 versicolor versicolor 5.4 3 4.5 1.5 versicolor versicolor 6 3.4 4.5 1.6 versicolor versicolor 6.7 3.1 4.7 1.5 versicolor versicolor 6.3 2.3 4.4 1.3 versicolor versicolor 5.6 3 4.1 1.3 versicolor versicolor 5.5 2.5 4 1.3 versicolor versicolor 5.5 2.6 4.4 1.2 versicolor versicolor 6.1 3 4.6 1.4 versicolor versicolor 5.8 2.6 4 1.2 versicolor versicolor 5 2.3 3.3 1 versicolor versicolor 5.6 2.7 4.2 1.3 versicolor versicolor 5.7 3 4.2 1.2 versicolor versicolor 5.7 2.9 4.2 1.3 versicolor versicolor 6.2 2.9 4.3 1.3 versicolor versicolor 5.1 2.5 3 1.1 versicolor virginica 5.7 2.8 4.1 1.3 versicolor versicolor 6.3 3.3 6 2.5 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 7.1 3 5.9 2.1 virginica virginica 6.3 2.9 5.6 1.8 virginica virginica 6.5 3 5.8 2.2 virginica virginica 7.6 3 6.6 2.1 virginica virginica 4.9 2.5 4.5 1.7 virginica versicolor 7.3 2.9 6.3 1.8 virginica virginica 6.7 2.5 5.8 1.8 virginica virginica 7.2 3.6 6.1 2.5 virginica virginica 6.5 3.2 5.1 2 virginica virginica 6.4 2.7 5.3 1.9 virginica virginica 6.8 3 5.5 2.1 virginica virginica 5.7 2.5 5 2 virginica virginica 5.8 2.8 5.1 2.4 virginica virginica 6.4 3.2 5.3 2.3 virginica virginica 6.5 3 5.5 1.8 virginica virginica 7.7 3.8 6.7 2.2 virginica virginica 7.7 2.6 6.9 2.3 virginica virginica 6 2.2 5 1.5 virginica versicolor 6.9 3.2 5.7 2.3 virginica virginica 5.6 2.8 4.9 2 virginica virginica 7.7 2.8 6.7 2 virginica virginica 6.3 2.7 4.9 1.8 virginica versicolor 6.7 3.3 5.7 2.1 virginica virginica 7.2 3.2 6 1.8 virginica virginica 6.2 2.8 4.8 1.8 virginica versicolor 6.1 3 4.9 1.8 virginica versicolor 6.4 2.8 5.6 2.1 virginica virginica 7.2 3 5.8 1.6 virginica virginica 7.4 2.8 6.1 1.9 virginica virginica 7.9 3.8 6.4 2 virginica virginica 6.4 2.8 5.6 2.2 virginica virginica 6.3 2.8 5.1 1.5 virginica versicolor 6.1 2.6 5.6 1.4 virginica virginica 7.7 3 6.1 2.3 virginica virginica 6.3 3.4 5.6 2.4 virginica virginica 6.4 3.1 5.5 1.8 virginica virginica 6 3 4.8 1.8 virginica versicolor 6.9 3.1 5.4 2.1 virginica virginica 6.7 3.1 5.6 2.4 virginica virginica 6.9 3.1 5.1 2.3 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 6.8 3.2 5.9 2.3 virginica virginica 6.7 3.3 5.7 2.5 virginica virginica 6.7 3 5.2 2.3 virginica virginica 6.3 2.5 5 1.9 virginica virginica 6.5 3 5.2 2 virginica virginica 6.2 3.4 5.4 2.3 virginica virginica 5.9 3 5.1 1.8 virginica virginica python corrects = dataset_test.loc[dataset_test['class'] == dataset_test['predicted class (by predict())']].shape[0] print('Prediksi Training Bayes: %d of %d == %f %%' % (corrects, len(data), corrects / len(data) * 100)) Prediksi Training Bayes: 141 of 150 == 94.000000 % MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Kesimpulan\u00b6"},{"location":"selection/","text":"Feature Selection \u00b6 feature selection atau seleksi fitur adalah sebuah proses yang biasa digunakan pada Machine Learning dimana sekumpulan dari fitur yang dimiliki oleh data digunakan untuk pembelajaran algoritma. Feature selection menurut Oded maimon telah menjadi bidang penelitian aktif dalam pengenalan pola, statistik, dan Data mining. Ide utama dari Feature selection adalah memilih subset dari fitur yang ada tanda transformasi karena tidak semua fitur /atribut relevan dengan masalah. Bahkan beberapa dari fitur atau atribut tersebut mengganggu dan mengurangi akurasi. Noisy Features atau fitur yang tidak terpakai tersebut harus dihapus untuk meningkatkan akurasi. Selain itu dengan fitur atau atribut yang sangat banyak akan memperlambat proses komputasi. Program Feature Selection \u00b6 from pandas import * from IPython.display import HTML , display from tabulate import tabulate from math import log from sklearn.feature_selection import mutual_info_classif def table ( df ): display ( HTML ( tabulate ( df , tablefmt = 'html' , headers = 'keys' , showindex = False ))) df = read_csv ( 'data_layla.csv' , usecols = [ 0 , 1 , 2 , 3 , 4 ], sep = ';' ) table ( df ) dibawah ini merupakan Hasil dari beberapa sampel yang telah ditentukan sebelumnya. outlook temperature humidity windy play sunny hot high False no sunny hot high True no overcast hot high False yes rainy mild high False yes rainy cool normal False yes rainy cool normal True no overcast cool normal True yes sunny mild high False no sunny cool normal False yes rainy mild normal False yes sunny mild normal True yes overcast mild high True yes overcast hot normal False yes rainy mild high True no Entropy \u00b6 Entropy adalah sebuah kolom yang mengandung niali-nilai bersih dari sebuah banyaknya kolom, entropy ini biasanya merupakan kolom yang mewakili pernyataan-pernyataan dari semua kolom . Rumus : $$ E(T) = \\sum_{i=1}^n {-P_i\\log{P_i}} $$ $$ P = merupakan~ probability~ yang ~mucul~ dalam ~row $$ def findEntropy ( column ): rawGroups = df . groupby ( column ) targetGroups = [[ key , len ( data ), len ( data ) / df [ column ] . size ] for key , data in rawGroups ] targetGroups = DataFrame ( targetGroups , columns = [ 'value' , 'count' , 'probability' ]) return sum ([ - x * log ( x , 2 ) for x in targetGroups [ 'probability' ]]), targetGroups , rawGroups entropyTarget , groupTargets , _ = findEntropy ( 'play' ) table ( groupTargets ) print ( 'entropy target =' , entropyTarget ) Tabel entropy yang sudah dicari sebelumnya value count probability no 5 0.357143 yes 9 0.642857 Hasil entropy entropy target = 0.9402859586706309 Gain \u00b6 Merupakan sebuah fitur yang berada dalam sebuah data Rumus mencari Gian \u00b6 $$ \\operatorname{Gain}(T, X) = \\operatorname{Entropy}(T) - \\sum_{v\\in{T}} \\frac{T_{X,v}}{T} E(T_{X,v}) $$ Program \u00b6 def findGain ( column ): entropyOutlook , groupOutlooks , rawOutlooks = findEntropy ( column ) table ( groupOutlooks ) gain = entropyTarget - sum ( len ( data ) / len ( df ) * sum ( - x / len ( data ) * log ( x / len ( data ), 2 ) for x in data . groupby ( 'play' ) . size ()) for key , data in rawOutlooks ) print ( \"gain of\" , column , \"is\" , gain ) return gain gains = [[ x , findGain ( x )] for x in [ 'outlook' , 'temperature' , 'humidity' , 'windy' ]] Outlook \u00b6 value count probability overcast 4 0.285714 rainy 5 0.357143 sunny 5 0.357143 gain of outlook is 0.2467498197744391 Temperature \u00b6 value count probability cool 4 0.285714 hot 4 0.285714 mild 6 0.428571 gain of temperature is 0.029222565658954647 Humidity \u00b6 value count probability high 7 0.5 normal 7 0.5 gain of humidity is 0.15183550136234136 Windy \u00b6 value count probability False 8 0.571429 True 6 0.428571 gain of windy is 0.04812703040826927 Program menyatukan semua Gain: \u00b6 table ( DataFrame ( gains , columns = [ \"Feature\" , \"Gain Score\" ]) . sort_values ( \"Gain Score\" )[:: - 1 ]) Feature Gain Score outlook 0.24675 humidity 0.151836 windy 0.048127 temperature 0.0292226 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Feature selection"},{"location":"selection/#feature-selection","text":"feature selection atau seleksi fitur adalah sebuah proses yang biasa digunakan pada Machine Learning dimana sekumpulan dari fitur yang dimiliki oleh data digunakan untuk pembelajaran algoritma. Feature selection menurut Oded maimon telah menjadi bidang penelitian aktif dalam pengenalan pola, statistik, dan Data mining. Ide utama dari Feature selection adalah memilih subset dari fitur yang ada tanda transformasi karena tidak semua fitur /atribut relevan dengan masalah. Bahkan beberapa dari fitur atau atribut tersebut mengganggu dan mengurangi akurasi. Noisy Features atau fitur yang tidak terpakai tersebut harus dihapus untuk meningkatkan akurasi. Selain itu dengan fitur atau atribut yang sangat banyak akan memperlambat proses komputasi.","title":"Feature Selection"},{"location":"selection/#program-feature-selection","text":"from pandas import * from IPython.display import HTML , display from tabulate import tabulate from math import log from sklearn.feature_selection import mutual_info_classif def table ( df ): display ( HTML ( tabulate ( df , tablefmt = 'html' , headers = 'keys' , showindex = False ))) df = read_csv ( 'data_layla.csv' , usecols = [ 0 , 1 , 2 , 3 , 4 ], sep = ';' ) table ( df ) dibawah ini merupakan Hasil dari beberapa sampel yang telah ditentukan sebelumnya. outlook temperature humidity windy play sunny hot high False no sunny hot high True no overcast hot high False yes rainy mild high False yes rainy cool normal False yes rainy cool normal True no overcast cool normal True yes sunny mild high False no sunny cool normal False yes rainy mild normal False yes sunny mild normal True yes overcast mild high True yes overcast hot normal False yes rainy mild high True no","title":"Program Feature Selection"},{"location":"selection/#entropy","text":"Entropy adalah sebuah kolom yang mengandung niali-nilai bersih dari sebuah banyaknya kolom, entropy ini biasanya merupakan kolom yang mewakili pernyataan-pernyataan dari semua kolom . Rumus : $$ E(T) = \\sum_{i=1}^n {-P_i\\log{P_i}} $$ $$ P = merupakan~ probability~ yang ~mucul~ dalam ~row $$ def findEntropy ( column ): rawGroups = df . groupby ( column ) targetGroups = [[ key , len ( data ), len ( data ) / df [ column ] . size ] for key , data in rawGroups ] targetGroups = DataFrame ( targetGroups , columns = [ 'value' , 'count' , 'probability' ]) return sum ([ - x * log ( x , 2 ) for x in targetGroups [ 'probability' ]]), targetGroups , rawGroups entropyTarget , groupTargets , _ = findEntropy ( 'play' ) table ( groupTargets ) print ( 'entropy target =' , entropyTarget ) Tabel entropy yang sudah dicari sebelumnya value count probability no 5 0.357143 yes 9 0.642857 Hasil entropy entropy target = 0.9402859586706309","title":"Entropy"},{"location":"selection/#gain","text":"Merupakan sebuah fitur yang berada dalam sebuah data","title":"Gain"},{"location":"selection/#rumus-mencari-gian","text":"$$ \\operatorname{Gain}(T, X) = \\operatorname{Entropy}(T) - \\sum_{v\\in{T}} \\frac{T_{X,v}}{T} E(T_{X,v}) $$","title":"Rumus mencari Gian"},{"location":"selection/#program","text":"def findGain ( column ): entropyOutlook , groupOutlooks , rawOutlooks = findEntropy ( column ) table ( groupOutlooks ) gain = entropyTarget - sum ( len ( data ) / len ( df ) * sum ( - x / len ( data ) * log ( x / len ( data ), 2 ) for x in data . groupby ( 'play' ) . size ()) for key , data in rawOutlooks ) print ( \"gain of\" , column , \"is\" , gain ) return gain gains = [[ x , findGain ( x )] for x in [ 'outlook' , 'temperature' , 'humidity' , 'windy' ]]","title":"Program"},{"location":"selection/#outlook","text":"value count probability overcast 4 0.285714 rainy 5 0.357143 sunny 5 0.357143 gain of outlook is 0.2467498197744391","title":"Outlook"},{"location":"selection/#temperature","text":"value count probability cool 4 0.285714 hot 4 0.285714 mild 6 0.428571 gain of temperature is 0.029222565658954647","title":"Temperature"},{"location":"selection/#humidity","text":"value count probability high 7 0.5 normal 7 0.5 gain of humidity is 0.15183550136234136","title":"Humidity"},{"location":"selection/#windy","text":"value count probability False 8 0.571429 True 6 0.428571 gain of windy is 0.04812703040826927","title":"Windy"},{"location":"selection/#program-menyatukan-semua-gain","text":"table ( DataFrame ( gains , columns = [ \"Feature\" , \"Gain Score\" ]) . sort_values ( \"Gain Score\" )[:: - 1 ]) Feature Gain Score outlook 0.24675 humidity 0.151836 windy 0.048127 temperature 0.0292226 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Program menyatukan semua Gain:"},{"location":"tugas1/","text":"Mean, Modus dan Median \u00b6 Rumus Mean (Rata-rata) Data Kelompok \u00b6 Untuk dapat menentukan mean atau rata rata dari data kelompok maka kita perlu menjumlahkan semua data kemudian membaginya dengan banyaknya data tersebut.namun, karena penyajian data kelompok tersebut diberikan dalam bentuk yang berbeda, maka rumus untuk mencari nilai mean (rata rata) untuk data kelompok itu terlihat sedikit berbeda dengan cara mencari nilai mean (rata rata) pada data tunggal. Rumus mean data kelompok dinyatakan dengan persamaan seperti di bawah. $$ \\bar x ={\\sum \\limits_{i=1}^{n} x_i \\over N} = {x_1 + x_2 + x_3 + ... + x_n \\over N} $$ Rumus Median Data Kelompok \u00b6 Median ialah data tengah setelah diurutkan. Pada data tunggal, nilai median tersebut dapat dicari dengan mengurutkan datanya terlebih dahulu kemudian mencari data yang terletak tepat di tengahnya.cara ini Hampir sama dengan cara mencari median pada data tunggal, nilai median pada data kelompok juga merupakan nilai tengah dari suatu kumpulan data. Karena bentuk penyajian datanya disajikan dalam bentuk kelompok,maka datanya tidak dapat diurutkan seperti pada data tunggal. Dengan demikian, agar dapat mencari nilai median dari suatu data kelompok diperlukan sebuah rumus. Rumus median data kelompok ialah sebagai berikut. $$ Me=Q_2 =\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ $$ Me=Q_2 =\\left( \\begin{matrix} {xn \\over 2 } {xn+1\\over 2} \\over 2 \\end{matrix} \\right), jika\\quad n\\quad genap $$ Rumus Modus Data Kelompok \u00b6 Modus ialah nilai data yang paling sering muncul atau data yang memiliki nilai frekuensi paling tinggi.untuk mencari nilai modus pada data tunggal sangat mudah,yaitu dengan Cara mencari nilai data dengan frekuensi paling banyak.namun untuk mencari mencari nilai modus pada data kelompok tidak lah semudah kita mencari nilai modus pada data tunggal. Hal ini dikarenakan bentuk penyajian data kelompok yang disajikan dalam sebuah rentang kelas. Sehingga, nilai modus data kelompok tidak mudah untuk langsung didapatkan dan untuk menemukan nilai modus dari data kelompok maka kita perlu menggunakan sebuah rumus. Rumus modus data kelompok dapat dilihat seperti persamaan di bawah ini. $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$ Variansi dan Standar Deviasi \u00b6 Variansi dan standar deviasi adalah ukuran penyebaran data. Nilai-nilai tersebut menunjukkan bagaimana penyebaran distribusi data. Standar Deviasi yang rendah berarti bahwa pengamatan data cenderung sangat dekat dengan rata-rata, sedangkan deviasi standar yang tinggi menunjukkan data tersebar di sejumlah nilai-nilai besar. Varian dari pengamatan N,x1,x2,...,xN, untuk atribut numerik X adalah $$ \\sigma ^ { 2 } = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } ( x _ { i } - \\overline { x } ) ^ { 2 } = ( \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } x _ { i } ^ { 2 } ) - \\overline { x } ^ { 2 } $$ Skewness \u00b6 Derajat distorsi dari kurva lonceng simetris atau distribusi normal. Ini mengukur kurangnya simetri dalam distribusi data Untuk menghitung derajat distorisi dapat menggunakan Koefisien Kemencengan Pearson yang diperoleh dengan menggunakan nilai selisih rata-rata dengan modus dibagi simpangan baku. Koefisien Kemencengan Pearson dirumuskan sebagai berikut $$ s k=\\frac{\\overline{X}-M o}{s} $$ dengan $$ \\overline{X}-M o \\approx 3(\\overline{X}-M e) $$ maka $$ s k \\approx \\frac{3(\\overline{X}-M e)}{s} $$ Tugas import pandas as pd from scipy import stats df = pd . read_csv ( 'data.csv' , sep = \";\" ) data = { \"stats\" :[ 'min' , 'max' , 'Mean' , 'Standart Deviasi' , 'Variasi' , 'Skewnes' , 'Quartile 1' , 'Quartile 2' , 'Quartile 3' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . min (), df [ i ] . max (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), round ( df [ i ] . skew (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.5 ), df [ i ] . quantile ( 0.75 ), df [ i ] . mean (), stats . mode ( df [ i ]) . mode [ 0 ]] kd = pd . DataFrame ( data ) kd . style . hide_index () MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Statistik deskriptif"},{"location":"tugas1/#mean-modus-dan-median","text":"","title":"Mean, Modus dan Median"},{"location":"tugas1/#rumus-mean-rata-rata-data-kelompok","text":"Untuk dapat menentukan mean atau rata rata dari data kelompok maka kita perlu menjumlahkan semua data kemudian membaginya dengan banyaknya data tersebut.namun, karena penyajian data kelompok tersebut diberikan dalam bentuk yang berbeda, maka rumus untuk mencari nilai mean (rata rata) untuk data kelompok itu terlihat sedikit berbeda dengan cara mencari nilai mean (rata rata) pada data tunggal. Rumus mean data kelompok dinyatakan dengan persamaan seperti di bawah. $$ \\bar x ={\\sum \\limits_{i=1}^{n} x_i \\over N} = {x_1 + x_2 + x_3 + ... + x_n \\over N} $$","title":"Rumus Mean (Rata-rata) Data Kelompok"},{"location":"tugas1/#rumus-median-data-kelompok","text":"Median ialah data tengah setelah diurutkan. Pada data tunggal, nilai median tersebut dapat dicari dengan mengurutkan datanya terlebih dahulu kemudian mencari data yang terletak tepat di tengahnya.cara ini Hampir sama dengan cara mencari median pada data tunggal, nilai median pada data kelompok juga merupakan nilai tengah dari suatu kumpulan data. Karena bentuk penyajian datanya disajikan dalam bentuk kelompok,maka datanya tidak dapat diurutkan seperti pada data tunggal. Dengan demikian, agar dapat mencari nilai median dari suatu data kelompok diperlukan sebuah rumus. Rumus median data kelompok ialah sebagai berikut. $$ Me=Q_2 =\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ $$ Me=Q_2 =\\left( \\begin{matrix} {xn \\over 2 } {xn+1\\over 2} \\over 2 \\end{matrix} \\right), jika\\quad n\\quad genap $$","title":"Rumus Median Data Kelompok"},{"location":"tugas1/#rumus-modus-data-kelompok","text":"Modus ialah nilai data yang paling sering muncul atau data yang memiliki nilai frekuensi paling tinggi.untuk mencari nilai modus pada data tunggal sangat mudah,yaitu dengan Cara mencari nilai data dengan frekuensi paling banyak.namun untuk mencari mencari nilai modus pada data kelompok tidak lah semudah kita mencari nilai modus pada data tunggal. Hal ini dikarenakan bentuk penyajian data kelompok yang disajikan dalam sebuah rentang kelas. Sehingga, nilai modus data kelompok tidak mudah untuk langsung didapatkan dan untuk menemukan nilai modus dari data kelompok maka kita perlu menggunakan sebuah rumus. Rumus modus data kelompok dapat dilihat seperti persamaan di bawah ini. $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$","title":"Rumus Modus Data Kelompok"},{"location":"tugas1/#variansi-dan-standar-deviasi","text":"Variansi dan standar deviasi adalah ukuran penyebaran data. Nilai-nilai tersebut menunjukkan bagaimana penyebaran distribusi data. Standar Deviasi yang rendah berarti bahwa pengamatan data cenderung sangat dekat dengan rata-rata, sedangkan deviasi standar yang tinggi menunjukkan data tersebar di sejumlah nilai-nilai besar. Varian dari pengamatan N,x1,x2,...,xN, untuk atribut numerik X adalah $$ \\sigma ^ { 2 } = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } ( x _ { i } - \\overline { x } ) ^ { 2 } = ( \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } x _ { i } ^ { 2 } ) - \\overline { x } ^ { 2 } $$","title":"Variansi dan Standar Deviasi"},{"location":"tugas1/#skewness","text":"Derajat distorsi dari kurva lonceng simetris atau distribusi normal. Ini mengukur kurangnya simetri dalam distribusi data Untuk menghitung derajat distorisi dapat menggunakan Koefisien Kemencengan Pearson yang diperoleh dengan menggunakan nilai selisih rata-rata dengan modus dibagi simpangan baku. Koefisien Kemencengan Pearson dirumuskan sebagai berikut $$ s k=\\frac{\\overline{X}-M o}{s} $$ dengan $$ \\overline{X}-M o \\approx 3(\\overline{X}-M e) $$ maka $$ s k \\approx \\frac{3(\\overline{X}-M e)}{s} $$ Tugas import pandas as pd from scipy import stats df = pd . read_csv ( 'data.csv' , sep = \";\" ) data = { \"stats\" :[ 'min' , 'max' , 'Mean' , 'Standart Deviasi' , 'Variasi' , 'Skewnes' , 'Quartile 1' , 'Quartile 2' , 'Quartile 3' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . min (), df [ i ] . max (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), round ( df [ i ] . skew (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.5 ), df [ i ] . quantile ( 0.75 ), df [ i ] . mean (), stats . mode ( df [ i ]) . mode [ 0 ]] kd = pd . DataFrame ( data ) kd . style . hide_index () MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Skewness"}]}